{"meta":{"title":"Down to the Rabbit Hole","subtitle":"放映机 方向键 二进制 多巴胺","description":null,"author":"crvdgc","url":"https://crvdgc.github.io"},"pages":[{"title":"about","date":"2018-07-24T08:51:30.000Z","updated":"2018-07-28T17:39:28.000Z","comments":true,"path":"about/index.html","permalink":"https://crvdgc.github.io/about/index.html","excerpt":"","text":"Down to the Rabbit Hole 兔子洞中到底有什么呢？ 这是一个科幻＋计算机爱好者的小站。 关注二进制、放映机、方向键和多巴胺的一切。 联系方式 Github: https://www.github.com/crvdgc/ 微信: yokis1997 邮件（推荐）: yokis1997@pku.edu.cn 友情链接： 微信公众号： 老火箭酒吧 laorocketbar"}],"posts":[{"title":"九章：真的是快一百亿倍的量子计算机吗","slug":"jiuzhang","date":"2020-12-06T19:32:58.000Z","updated":"2020-12-06T19:38:00.925Z","comments":true,"path":"2020/12/07/jiuzhang/","link":"","permalink":"https://crvdgc.github.io/2020/12/07/jiuzhang/","excerpt":"","text":"据新华网报道，中科大潘建伟团队制作的新量子计算机九章，比谷歌量子计算机快一百亿倍。本文将从计算理论的角度来说明，该如何理解这一结论。 首先一个重要的问题是，九章是量子计算机吗？九章的确有电子元件，并且使用了量子效应，可以执行一个量子算法，为何不能称为量子计算机呢？ 要回答这个问题，首先可以考虑一下，耳机是计算机吗？毕竟耳机内部也包含电子元件，并且完成了将电流信号转化成声音信号的算法，但我们通常不将耳机称为电子计算机，而只是电子机器。是什么将耳机和电脑区别开来呢？ 我们知道，不同的机器可以执行不同的功能。我们的生活中有很多专用机器，比如收银机、汽车的刹车辅助系统、导弹的制导系统等等。它们都有不同的结构，完成各自的专用任务。如果没有一项发现的话，也许人们直到现在也得为每一项不同的任务设计一个新的专用机器。1933年前后，英国数学家阿兰·图灵以及其他数学家意识到，在不同的计算任务背后，隐藏着一个相同的抽象结构[1]。对这种抽象结构的各种表述中，以图灵的图灵机最广为人知。 简单来说，图灵机就是一个概念上的机器，根据自身的结构，在一个无限长的纸带上完成读写信息的任务。一个最简单的图灵机例子就是计算生成1/3的小数表示，只需按下面两个步骤执行即可： 在纸带上打印「0」和小数点。 不断在纸带上打印「3」。 运行这个机器，就可以在纸带上得到结果「0.3333……」。当然，图灵所使用的模型更加抽象，只包含一些基本操作，并且使用二进制编码，但这些都不影响原理上的讨论。图灵在论文中还给出了一个计算圆周率π的图灵机。每一个的图灵机就如同现实中的专用机器一样，它们都有自己的专门任务。正如一个收银机无法完成导弹的制导一样，一个计算1/3的图灵机，无法完成计算π的任务。 但图灵接下来的结论让人十分震惊：通过对图灵机的操作也进行编码，图灵将每个专用的图灵机都转化成了一系列指令。接着，他证明了存在一个通用图灵机（Universal Turing Machine），只要把某个专用图灵机对应的指令放进去，它就能表现得像那个专用图灵机一样。换句话说，通用图灵机可以模拟任何的专用图灵机，因此才被称为是通用的（universal）。 这个结果其实十分有趣。通用图灵机可以用很少的部件制作而成（事实上，图灵在论文中给出了一个完整构造，从而证明了通用图灵机的存在），但却能「模仿」无穷无尽的专用图灵机。现在的每个人对这些概念其实十分熟悉：因为每一个电脑、手机都是通用图灵机，而不同的软件就是不同专用图灵机的指令。电脑不仅可以计算1/3和π，还能通过执行音乐播放器软件，做到原来一台专门的MP3机器才能做到的事情。对此，人们已经习以为常，并不觉得奇怪。但如果仔细考虑一下的话，电脑的灵活性的确很令人惊讶。 这也就回答了上面，耳机和电脑之间的区别。只有当一台机器能执行通用计算任务的时候，我们才将它称为计算机。耳机虽然能高效地完成一项特定的计算任务，但因为不具有通用性，我们并不将其称为计算机。一个更贴近生活的例子是，印章可以快速地复制一个图案，但如果想要复制任意图案的话，还是需要一台通用性更强的复印机。 类似的，九章其实是一个量子机器，而不是一个量子计算机。为何这么说呢？ 对应于经典算法，计算机科学家为量子计算任务设计了一套「编程语言」，称为量子电路（quantum circuit）。令人们感兴趣的量子计算任务，都可以使用一个量子电路来描述。当然，我们可以将每一个量子电路都实际用器件搭出来，形成一个专门的物理量子电路，但这样就太麻烦了。 在制作经典电路时，计算机科学家为了简化器件的种类，发现了任何逻辑电路都可以使用与、或、非三种逻辑门来实现。换句话说，只需要做出三种器件，再用一定的方式将它们组合到一起，就可以完成任意逻辑电路的功能。有了任意的逻辑电路，就可以很轻松地做出物理上的通用图灵机了。接下来，如果想让这台机器解决什么问题，我们只需要为它编程就好。 类似的，计算机科学家发现，任意的量子电路都可以转化为由单量子比特门和 CNOT 门构成的电路[2]。也就是说，只需要做出能实现这两个门的器件，再通过一定方式将它们组合到一起，就可以实现通用的量子计算。这就是开头那篇新闻报道中所说的，谷歌的量子计算机实现的「随机线路取样」。实际上，任何其他的量子计算机，指的都是可以做到执行任意量子电路的计算系统。很少有人将不能进行通用量子计算的系统，称为「量子计算机」。 那么九章用来展示「量子优越性」（quantum supremacy）的「高斯玻色取样」是什么呢？简单来说，就是预测一个光量子系统演化的结果[3]。用经典计算机模拟量子系统十分复杂，因此量子计算机概念最早提出者之一的物理学家理查德·费曼就设想，量子计算机的一大应用就是模拟量子系统本身。但问题的关键在于，拥有一台通用的量子计算机，可以模拟任意专门的量子系统（只不过速度可能很慢），但拥有一台专门的模拟一种量子系统的机器，并不等于能完成其他的量子计算。 就我个人理解，并非任意量子电路都可以转化为「高斯玻色取样」。因此，「悬铃木」以及其他所有量子计算机的通用性，都严格高于九章。事实上，我不知道任何有意思的问题（比如密码学、机器学习）可以编码成为「高斯玻色取样」问题，而它们全都能编码成为通用的量子电路，并可以在量子计算机上运行。因此，很可能九章只是再次证明了这样一个事实：用经典计算机模拟量子系统很困难。这一事实虽然广为人知，但这次使用实验证明了这一点。九章选取了一个困难、但可能没有应用价值的问题，证明了它的确很困难。 举例来说就是，对于下面的弹球机，请预测不同力度弹出的小球最终的结果如何？ 如果使用计算机模拟的话，可能需要仔细建模小球、弹杆、还有机器内部的状态等等，十分复杂。另一个更「高效」的计算方法是，直接拿一台这样的机器，用给定的力度弹出，最后汇报结果。后一种方法可能的确比前面的快很多，但这个问题本身只是说明了物理系统模拟的困难性。并不是任意问题都可以编码成弹球机的小球结果。 报道中称，「无论是谷歌的『悬铃木』处理『随机线路取样』，还是『九章』求解『高斯玻色取样』，都只能用来解决某一个特定问题」，但我们可以看到，两个问题的重要性不可同日而语。毕竟我们也可以说，电脑和耳机都只能用来解决某一个特定问题：耳机只能将电信号转化为声音信号，电脑不过是可以通过重新编程模拟任何计算系统而已。 而报道中的另一个引人注目的数字是，「『九章』比『悬铃木』快100亿倍」。那么这个数字是怎么算出来的呢？报道中说，是与「最快的超算等效比较」而得出的。也就是说，如果「悬铃木」比某一个超算快N倍的话，九章则快100亿N倍。但上面的分析已经指出，二者并没有从事相同的计算任务，因此这样的比较很难有什么实际意义。比如，我们可以选取一个无法被编码为「高斯玻色取样」的量子计算问题，比如 Shor 算法[4]，它可以在「悬铃木」上经过一段时间得出结果。但因为九章并非通用量子计算机，它根本无法解决这个问题，此时讨论速度意义不大。 总而言之，「『九章』比『悬铃木』快100亿倍」，就好像在说，印章的速度比复印机快十倍一样。对于复制印章上的图案而言，的确如此。但后者的通用性远高于前者，而且前者的印章印出来的还很可能是一个没人需要的图案。 Turing, Alan Mathison. “On computable numbers, with an application to the Entscheidungsproblem.” Proceedings of the London mathematical society 2.1 (1937): 230-265. ↩ Nielsen, Michael A., and Isaac Chuang. “Quantum computation and quantum information.” (2002). ↩ https://www.sciencenews.org/article/new-light-based-quantum-computer-jiuzhang-supremacy ↩ Shor, Peter W. “Algorithms for quantum computation: discrete logarithms and factoring.” Proceedings 35th annual symposium on foundations of computer science. Ieee, 1994. ↩","categories":[{"name":"科普","slug":"科普","permalink":"https://crvdgc.github.io/categories/科普/"},{"name":"计算机","slug":"科普/计算机","permalink":"https://crvdgc.github.io/categories/科普/计算机/"}],"tags":[{"name":"新闻","slug":"新闻","permalink":"https://crvdgc.github.io/tags/新闻/"},{"name":"量子计算","slug":"量子计算","permalink":"https://crvdgc.github.io/tags/量子计算/"},{"name":"计算理论","slug":"计算理论","permalink":"https://crvdgc.github.io/tags/计算理论/"}]},{"title":"信条：本可以更自由的点子科幻","slug":"tenet","date":"2020-12-05T13:57:00.000Z","updated":"2020-12-05T14:06:10.298Z","comments":true,"path":"2020/12/05/tenet/","link":"","permalink":"https://crvdgc.github.io/2020/12/05/tenet/","excerpt":"","text":"本文将探讨科幻电影「信条」的核心设定有哪些和现实中的物理学与哲学兼容，哪些不兼容；以及作为点子科幻有哪些有趣的地方。 时间旅行 「信条」的核心设定是时间旅行，也就是使用某种机制，让人回到过去。以往的时间旅行都是瞬间完成的，但「信条」中，向过去穿越的过程变成了一种时间反演。处在中间状态的人，还可以和周围环境互动。为了方便后面的讨论，我们借用电影中的方法，把向时间正向演进的物质称为红物质，而反向演进的物质称为蓝物质。 本体论：红蓝物质≠正反物质 现实世界中是否有和电影中类似的蓝物质呢？ 对应每一种基本粒子，都存在反粒子。比如电子和反电子。反电子带正电，与电子相遇时会发生湮灭（annihilation），转化为光子。有的物理学家认为，反粒子是在时间上向相反方向行进的粒子。而且电影中也提到，当蓝人遇到过去的自己时，也会发生湮灭（annihilation）。 但其实蓝物质并非反物质。否则的话，蓝人即使戴上呼吸面罩，呼吸面罩的表面、衣服的表面等等，都会和无处不在的物质发生湮灭。蓝人会在出去的一瞬间化成光而消失，就更不用说打架之类的事了。因此红蓝物质并非正反物质。 蓝物质是一种新的物质。它的性质如何呢？ 从电影来看，除了时间反演之外，蓝物质与红物质遵循类似的物理规律。比如我们想研究两个蓝色小球相撞之后会怎样，那结果一定等同于两个红色小球相撞，再将时间反演的结果。 但上面的结论只适合于全红色或者全蓝色物质，如果红蓝物质之间相作用，会发生什么呢？电影似乎想说，蓝物质与红物质会遵循相同的物理定律。比如牛顿第三定律，因此蓝人在击打红人时，红人会受力后退。 但其实，红蓝物质相互作用遵循的规律和现有的物理规律可以完全不同。因为我们现有的一切物理规律，都是靠研究红物质和红物质作用得出的。通过时间反演，我们只能得到蓝物质和蓝物质之间的作用规律。红蓝物质相遇会发生什么，完全无法预料。 电影也隐约意识到了这点，因此要求蓝人不能呼吸红空气。但蓝汽油却可以在红空气中点燃，最终造成结冰。这看起来像是个漏洞。因为我们知道，红汽油在红空气中点燃，物体会从原有温度上升。通过时间反演，可以得出，蓝汽油在蓝空气中点燃，物体会从高温下降到原有温度，而非比原来更低。但因为我们并不知道蓝汽油在红空气中点燃会发生什么，完全可以设定这个过程从原有温度开始降温。 但话又说回来，蓝汽车内，同样发生了蓝汽油在红空气中燃烧的反应，按上面的理解，汽缸应该被冻住，而非空气膨胀驱动汽车前进。也就是说，如果想开蓝车，必须像蓝人带呼吸面罩一样，给车装一个供蓝氧气的瓶子。 从这个例子中可以看出，红蓝物质之间的相互作用完全无法预料，因此电影其实可以任意规定。 形而上学：红蓝世界所遵循的物理法则 在本体论的基础上，红蓝世界所遵循的物理法则是怎样的呢？可以说，这是一个很奇怪的世界。下面三条结论都表明红蓝世界与纯红世界的不同之处： 因果不存在 概率不存在 量子力学不成立 相对论不成立 因果不存在，是因为一般认为因果关系要求时间上的先后。但通过红蓝转换，人主观上经历的时间与世界时间不一样，因此人总结的因果，并不一定在世界中有时间先后的关系。 概率不存在，可以用下面的思维实验证明：开始时，一位红人投一枚硬币。一位蓝人观察结果后，回到红人投硬币之前，记录下来。我们总可以使用记录来准确预测投硬币的结果，因此投硬币并非随机过程。 量子力学不成立，也可以用类似的方法。我们测量一个量子态，记录下结果，再回到测量之前。这个记录可以准确预测测量的结果。 相对论不成立，因为存在超距作用（signaling）。假设某地有一些信息，想以超光速的方式传送给另一地。可以这样做到：首先让一位红信使走完一半的路程，再让一位蓝信使以相同的速度走完剩下的路程。这样就完成了信息的超光速传送，而这是被相对论所禁止的。 认知论：在红蓝世界中思考 上面的法则可以推导出一个令人惊讶的结论，那就是红蓝世界中，自由意志并不存在。 就拿投硬币的实验来说。如果我们看到了投硬币结果的记录，那就知道红人一定会投硬币。现在设想红人同样看到了结果，其意识上发生了变化，但这种变化并不能通过意志来改变即将做出的决定，也就是说，无法通过意志选择不投硬币。因此红蓝世界中并不存在自由意志。实际上，这是一种非概率的机械决定世界。 从电影还可以得到，红蓝世界是一个二元论（dualism）的世界，也就是说，人的意识并不完全取决于物质基础。 当主角为了返回奥斯陆而进行转换时，我们看到，他的胳膊上出现了伤口。这是因为在之前的经历中（从电影来看仍未发生），他在打斗中被玻璃扎伤。但类似的，他的大脑神经元也应该在转换的一瞬间转到拥有这段记忆的状态。假如世界并非二元论，而是唯物的（materialistic），那他的主观体验应当是，转换的一瞬间获得所有记忆，然后记忆逐渐减少。因此这个世界是二元论的。 在一个物质时间倒转的世界中，人的意识体验是怎样的？这个问题完全无法回答。有人认为会倒转，也有人认为会完全一样。刘慈欣的短篇小说「坍缩」中，讨论了物质世界的时间倒转（来源于宇宙从膨胀转为坍缩）。结尾，他通过将倒数第二段倒过来的方式表现时间反转。但这只是身处宇宙外的旁观者才能看到的。宇宙内的意识到底会怎样，是完全无法回答的。 电影的选择 在上面的讨论中，红蓝物质相互作用，以及蓝人的意识体验，两者都是难以预料的。电影的选择是在日常生活的层面将概念取反来规定的。比如火会升温，因此蓝火会降温等等。其实规定并不严谨，比如蓝车的问题。 但这些问题并不妨碍这部作品成为一个优秀的点子科幻：不存在自由意志也就从根本上排除了故事的可能性，因为任何一个角色都无法做出自由选择。但它展示了一种异于现实的沙盒世界。可以提供很多思考的乐趣。 尽管有如此高的自由度，但「信条」最终对自己设定的使用还是趋于保守，并没有过度强调最有趣的后果（自由意志并不存在），只是在影片结尾时，暗示了主角领悟到了这一点。 总的来说，「信条」是一部本可以更自由的有趣的点子科幻。","categories":[{"name":"科幻","slug":"科幻","permalink":"https://crvdgc.github.io/categories/科幻/"},{"name":"评论","slug":"科幻/评论","permalink":"https://crvdgc.github.io/categories/科幻/评论/"}],"tags":[{"name":"电影","slug":"电影","permalink":"https://crvdgc.github.io/tags/电影/"}]},{"title":"人狼拓扑","slug":"jin-roh-topology","date":"2020-11-18T08:44:58.000Z","updated":"2020-11-18T08:49:09.239Z","comments":true,"path":"2020/11/18/jin-roh-topology/","link":"","permalink":"https://crvdgc.github.io/2020/11/18/jin-roh-topology/","excerpt":"","text":"《人狼》（1999）是一部由押井守编剧，冲浦启之导演的动画电影。影片围绕着一个 proto-fascist 社会中，两个政治势力之间的斗争展开。 故事开始于一次抗议，半军事化警察组织 Special Police Unit 与革命组织 The Section 之间发生了冲突。警员一贵在下水道追逐一位少女，并将她逼入绝路。少女引爆炸弹自杀，一贵因为未能即使开枪阻止而受到处罚。遇见少女的妹妹圭后，一贵开启了一段赎罪之旅。最终，圭向他表明，自己是被逮捕的革命组织成员。因长相相似炸弹少女而被 SPU 的对手 Public Security 机构用作诱饵，希望制造丑闻来除掉 SPU 。圭愿意和一贵一同逃走，但一贵表示自己其实是 SPU 中的卧底革命组织 The Wolf Brigade 的成员。他们是将计就计，借她之手除掉 PS 的成员。成功后，一贵在 TWB 的命令下，将圭杀害。 整个故事中穿插讲述了童话《小红帽与大灰狼》，着重强调「狼伪装成为人」这一 motif 。开始时，狼与人的界限似乎十分清晰。 $$\\begin{aligned} \\text{proto-fascist }政府 &amp;\\iff 抗议者 \\\\ 狼 &amp;\\iff 人 \\end{aligned}$$ 一贵的身份开始时是「狼」，但目睹了少女在他面前引爆炸弹之后，开始反思自己的行为。在和圭的接触中，逐渐表现出自己人性的一面。故事到这里，观众得到的印象是一个传统的人文主义桥段。即打破了「系统」的束缚，内心的人性得以表达的故事。 $$\\boxed{狼\\boxed{人}} \\rightarrow \\boxed{狼 | 人} \\dashrightarrow \\boxed{人}$$ 当圭表示自己实际上是诱饵时，人-狼之间的关系在圭身上得到了互文。圭愿意通过牺牲他人来拯救自己，而做到这一点的方式则是装成人的样子。 $$\\boxed{人} \\rightarrow \\boxed{人\\boxed{狼}} \\rightarrow \\boxed{狼}$$ 在营救行动后，圭表示一贵的出现让自己看到了出路，两人可以一同逃走，这说明她的内心深处仍然有人性的存在，因此仍然有获得救赎的可能： $$\\boxed{人\\boxed{狼}} \\rightarrow \\boxed{人\\boxed{狼\\boxed{人}}} \\rightarrow \\boxed{狼\\boxed{人}} \\rightarrow \\boxed{人}$$ 一贵最终揭示自己 TWB 的身份，其实一直是在利用圭，是真正的伪装成人的狼，因此断绝了救赎的可能，为故事带来了一个灰暗的结局： $$\\boxed{狼\\boxed{人\\boxed{狼}}} \\rightarrow \\boxed{人\\boxed{狼}} \\rightarrow \\boxed{狼}$$ 一贵曾梦到狼吃掉小红帽-少女-圭，在故事的开头可能理解为单纯的 proto-fascist 警察怀疑自己的行为是否道德。但最终的揭示让这个梦获得了新的含义，也就是圭成为了自己的牺牲品。 让我们在回到故事的开头，狼与人之间的界限似乎十分明显， $$\\begin{aligned} \\text{proto-fascist }政府 &amp;\\iff 抗议者、\\text{TWB} \\\\ 狼 &amp;\\iff 人 \\end{aligned}$$ 故事却揭露了这一对立的天真性。 TWB 与 proto-fascist 政府之间，立场相对，但所采取的手段却是相同的。即为了目标不择手段的马基雅维利式 power politics 。无论是政府官员、军警，还是 The Section 的抗议者、TWB ，他们在战斗时全都是面无表情的。他们将自己作为某种信念的容器，从而完全屈从于近乎本能的「兽性」，而停止了自己的思考与判断。人性在这场对立之中其实是缺失的： $$\\begin{aligned} \\text{proto-fascist }政府 &amp;\\iff 抗议者、\\text{TWB} \\\\ 狼_1 &amp;\\iff 狼_2 \\end{aligned}$$ 真正能体现出人性的部分，则是一贵与圭在整个过程中所体现的挣扎。虽然每个人都很清楚自己的目标，但是又从直觉上感受到了违反人性所带来的异常。如果是纯粹的狼，大概会毫无情感地完成一切，而不会有所烦恼吧。故事的结局似乎表示，人性所带来的挣扎仅仅在故事中才会起作用，而在现实中，只会被 power politics 所击败。 人们的目的当然是希望实现道德现实。一个不那么灰暗的解读是，影片指出了仅有道德直觉（人性）是不够的。一贵的狼性培养于警察学校，在那里只有两种可能，成为 proto-fascist 政府的狼，或者成为 TWB 的狼。个体性在这样的环境中被毁灭，区别只是要重新装入哪种思想。圭的两次转变则分别是青少年（未形成完整的政治判断）和监禁中。也许在不同的环境中，仍然会有希望。","categories":[{"name":"科幻","slug":"科幻","permalink":"https://crvdgc.github.io/categories/科幻/"},{"name":"评论","slug":"科幻/评论","permalink":"https://crvdgc.github.io/categories/科幻/评论/"}],"tags":[{"name":"动画","slug":"动画","permalink":"https://crvdgc.github.io/tags/动画/"},{"name":"电影","slug":"电影","permalink":"https://crvdgc.github.io/tags/电影/"},{"name":"政治","slug":"政治","permalink":"https://crvdgc.github.io/tags/政治/"}]},{"title":"魔法少女小圆：叛逆的物语 - 基督二性冲突","slug":"madoka-a-conflicting-theanthropic-christ","date":"2019-09-15T11:27:36.000Z","updated":"2020-11-21T10:41:26.380Z","comments":true,"path":"2019/09/15/madoka-a-conflicting-theanthropic-christ/","link":"","permalink":"https://crvdgc.github.io/2019/09/15/madoka-a-conflicting-theanthropic-christ/","excerpt":"","text":"「魔法少女小圆：叛逆的物语」剧情紧接「魔法少女小圆」 TV 版。如同「凉宫春日的消失」和菲利普·K·迪克的很多小说一样，整个故事的核心是世界的虚假性。首先开始于一个看似正常的世界，接下来逐渐展示其虚假之处，并探索原因。最终以世界的去向作为冲突的高潮，冲突解决于对世界虚假性的接受。 从 TV 版开始，「小圆」就有「神展开」（pun intended）的名声。这次的剧场版也不负众望。剧情上的意外，反映了本作对于类型作品的反思与颠覆。 TV 版打破了观众对「魔法少女」故事的预设，「虽然经历艰难，但只要坚持对美好事物的愿望，正义最终必将战胜邪恶，迎来圆满（pun intended）的结局」。虽然开始于这样的前题，但 TV 版还是落在了一个大团圆结局上，多少又回归到了传统之中。「不存在圆满的结局」，那么什么才是 TV 结局之中的不圆满之处呢？这正是「叛逆的物语」要回答的问题。 小圆与丘比之间的主要冲突，在于对「愿望」的不同态度。小圆肯定愿望，而丘比否定愿望。所谓的「愿望」，指的是「不合常理的希望」，而「常理」指的就是丘比所代表的「现实」以及认识现实的「科学方法」。 对现实的清醒认知，会使人像丘比一样冷漠、没有感情。如果沙耶香从现实的角度出发，理应认识到自己对上条恭介的鼓励是「不现实」的。因为上条恭介的病情取决于他的身体情况以及治疗手段，而不是沙耶香的愿望。沙耶香并没有足够的医学专业知识得出对方「一定会好起来」这样的结论。拥有或者表达这样的想法，都是一种极不合理的行为，是对现实视而不见。 当然，比起丘比，观众会认为，沙耶香的想法才是人之常情。人们既能看到现实，同时又拒绝现实，这是因为人们具有情感。从情感上来讲，人们愿意看到希望会对事情产生积极的影响。因此在类型故事中，为了回应这样的情感需求，美好的愿望往往能带来美好的结果。人们将能颠覆现实的东西称为魔法。因此，人们不会真正问魔法是怎样工作的，正如人们不会问故事所带来的「魔法」到底是否现实一样。 TV 版通过颠覆这种传统，向观众揭示了这种「故事逻辑」的不现实之处。观众对故事抱有希望，正如故事中的角色对结果抱有希望一样。故事通过出乎意料的情节展开，在两个层面上同时进行了否定。 但 TV 版的结局似乎又传达了相反的信息。小圆之所以能够胜过丘比，表面上是因为她提出了一个聪明的悖论。但试想，如果故事的最后是一个小圆之外的角色，因为刻苦学习逻辑学，提出了一个完全相同的愿望并取得胜利，还会达到原作的效果吗？从一开始，故事就在不断强调小圆只是一个「普通的中学生」，没有什么突出的优点。小圆之所以能够成为主角，并对丘比取得胜利，是因为她对「愿望」始终保持着肯定的态度，从未动摇。这就是她获胜的唯一原因。 在故事系统内的解释是，因为小圆身上累积了巨大的因果量，所以只有小圆能做到。而她之所以能积累如此大的因果量，是因为晓美焰一直对「希望」保持着肯定的态度，不断战斗。归根到底，结局所传达的信息，正是传统的故事逻辑，「坚持对美好事物的愿望，就能带来美好的结果」。 但的确是这样吗？ 让我们考察一下 TV 版中的丘比。丘比代表的是现实，或者说规则。故事只能产生于现实，因此魔法少女的力量全都来源于丘比。而现实并非是为了产生故事而存在的，故事只是现实法则的副产品。现实有其自己的逻辑。当二者发生冲突时，现实往往占上风——因为它是现实。就像是在后半段故事中所揭示的那样，丘比产生魔法少女只是为了从中获取能量。而如果这一过程需要让少女产生绝望，并事实上杀死魔法少女的话，丘比并没有任何额外的考虑。现实中，一种对奴隶制的辩护就使用了相同的逻辑，一些工作总要有人做，如果能让一些人完成的话，就不必顾虑可能产生的后果。即使这些后果包括让一些人失去包括自由的基本权利。 结局中，小圆所做的，表面上看起来是对所谓「现实逻辑」的否定。要注意到，丘比并非现实法则本身，它只是阐释了自己对法则的理解。从结局中我们可以看到，丘比对世界的理解是错误的。它代表的不是现实，而是另一种不同的故事。作为法则的创作者本身，编剧提供了一种丘比意料之外的解决方法。这也是为何人们可以反驳上述的观点，「一些工作总要有人去做」，其实是将一些未检验的假设当作了现实。现实证明，一些工作总要被完成，但未必需要人来完成。同理，即使承认丘比的目标，即阻止热寂，让少女产生绝望并非唯一的方式，甚至都不是最有效的方式。完全可以想象，在这一事件后，丘比决定量产「肯定希望」的魔法少女，收取能量来阻止热寂。 这一结局看似解决了冲突，但无论是「希望」还是「绝望」，二者都仍然遵循了「现实逻辑」，也就是将对现实的认知当作现实本身，只要能达到目的就可以不择手段，无论这个手段会产生好的副作用，还是坏的副作用。归根到底，它们仍然把「希望/魔法/故事逻辑」还原（reduce）成现实法则的副产品，没有承认「希望」本身有任何内在的意义。最能体现这一点的是， TV 版的结局中小圆选择了自杀，转化成了「圆环之理」，也就是一种法则，而使得其他人「得救」。这是对基督的人神二性的最基础的解释，当耶稣选择在十字架上自杀时，作为人的耶稣死去了，但他变成了圣灵（holy spirit），也就是一种鬼魂（holy ghost），从而能一直指导人们走向救赎。从现实逻辑来看，基督之死是对基督教的最大打击。但事实说明，所谓的现实逻辑并非现实。通过新法则的揭示，基督之死成了基督教走向世界宗教的开始。 真正「希望/魔法/故事逻辑」所带来的结局应当是这样的：即使肯定希望不会阻止热寂，乃至反而会加速热寂，小圆仍然坚持同样的抉择。这就无歧义的说明了，「希望」本身有其内在的意义，无需现实法则的给予选择的基础。这正是剧场版的主题。只不过做出这一选择的不是小圆，而是焰。 剧场版的故事分为两段。第一段是晓美焰解开了虚假世界的谜题——正如上面所说，丘比试图通过实验控制圆环之理。第二段则是结局的转折，即将作为人性的小圆和圆环之理分开，阻止自己和所有人得到「救赎」。中间的转折看似突然，实则是第一段故事的自然结果。据说剧场版的故事起源于编剧认为 TV 版的结尾中，小圆自杀有些过于残酷的感觉。 如果说 TV 版讲述的是基督受难的故事，剧场版则是文艺复兴的重新演绎。如果达到至善（使用圆环之理阻止热寂）的代价是失去人性（小圆自杀），那或许并不值得。因为人性（作为人的小圆）有其自身的价值，在这里，作为晓美焰的爱的对象，而非法则的副产品，因此在二者的冲突中，有可能会选择保持人性，而非达到至善。何况，所谓的至善也只是一种对法则的阐释。除非编剧外在地强行指定，又怎能确定圆环之理不会是另一个丘比的法则呢？ 然而这样的选择并非易事。毕竟，晓美焰是要「忤逆神意」。拒绝即使是未经确认的至善仍然会造成严重的结果。当走出了法则的确定的领域后，就会感受到「自由的眩晕」。整个第一段故事，则可以看作是晓美焰说服自己作出抉择的过程，或者说，让自己接受已经做出的抉择的过程。 在开始时，晓美焰甚至否认自己拥有这样的愿望。她让自己相信，产生这一愿望的不是自己。因此她忘记了创造虚假世界的正是自己。当真相被揭开时，她的反应不是后悔自己为何将小圆一行人困在里面，而是尝试去理解过去的自己，去寻找做出这一决定的原因。当她终于说服了自己后，便做出了抉择。只有经历了第一段的矛盾与挣扎，她才能在做出抉择后毫无悔过之意。而作为人的小圆仍然选择了“秩序”，则表明，人神二性分离的基督将失去全部的神性。文艺复兴时期，列奥纳多·达·芬奇所作的《施洗者圣约翰》，将为基督施洗的约翰描绘成了一个带有神秘微笑的乃至具有挑逗性意味的人。最终造成的结果是对人性的完全肯定，而非人神二性的结合。 奥斯卡·王尔德对圣经故事的重新阐释《莎乐美》也有类似的效果。因此，剧场版的结局或多或少带有一些后现代主义的先声：人神二性分离后，神性荡然无存（作为人的小圆拒绝了晓美焰），在这样世界中生活的人们只能创造自己存在的意义。晓美焰最终的舞蹈，则是对这一段同样艰难的过程的描述。最终晓美焰必须认识到，「对小圆的爱」也仅仅是一个借口，做出抉择的唯一原因只是自己。小圆的存在只是增加了一些帮助，但抉择的必要性则是内在的，即使痛苦也不得不做出。 尼采认为启蒙时代后，人们已经无法再让自己相信神性的存在。人们常常误会，尼采是兴高采烈地说出“上帝已死”，仿佛是在庆祝一场比赛的胜利一样。但他则看到了这一过程的痛苦之处，每个人都必须用巨大的勇气才能接受这一点，还需要付出更多努力才能从中看到新的可能性。也许在各种手段中，创造一整个虚假的世界远非最极端的一种。就让我以对尼采的完整引用结束这篇评论吧： 上帝死了。他会一直死下去。是我们把他杀死了。我们该如何安慰自己，这些犯下了所有凶杀中最严重凶杀的人。世界上最神圣的、最强大的存在就这样在我们的刀下流血致死：谁能将这血迹从我们的身上抹去？什么样的水能将我们洗净？我们必须为此要发明何种赎罪的仪式，怎样的神圣游戏？这件事中的伟大之处，对我们来说，难道不是过于伟大了吗？难道我们不是必须要让自身成为上帝，才足够弥补它的伟大吗？","categories":[{"name":"科幻","slug":"科幻","permalink":"https://crvdgc.github.io/categories/科幻/"},{"name":"评论","slug":"科幻/评论","permalink":"https://crvdgc.github.io/categories/科幻/评论/"}],"tags":[{"name":"动漫","slug":"动漫","permalink":"https://crvdgc.github.io/tags/动漫/"}]},{"title":"当我放下箱子的时候","slug":"when-i-put-down-my-suitcase","date":"2019-07-07T09:15:08.000Z","updated":"2019-07-08T03:32:07.484Z","comments":true,"path":"2019/07/07/when-i-put-down-my-suitcase/","link":"","permalink":"https://crvdgc.github.io/2019/07/07/when-i-put-down-my-suitcase/","excerpt":"","text":"当我放下箱子的时候，咖啡店里的一个人引起了我的注意。尽管我现在已经应该走在离开的路上了，但手腕的酸痛还是战胜了我。我只好放下箱子，休息一会。把行李放在路中间不是什么文明的行为，但我还是不顾路人紧皱的眉头，开始打量起了那个人。 他没戴眼镜，普通的发型。双手不停在笔记本上敲打着什么，不时和同桌的人聊上两句。桌面摆了几杯咖啡。这一切都稀松平常，每晚路过时，我甚至都不会注意到咖啡店的这扇窗户。但临行前，我在这里放下了箱子，看了一眼，结果却令我大吃一惊，那个人竟然和我刚来的时候长得一模一样。 喂，你该走了，我想对他说，你已经不属于这里了。看，你的行李都被我打包好，装进了这个箱子。你在这里什么都没有了，也没什么好待的了，不是吗？ 他在那里认真地写着什么。仔细看看，他身边的人我一个也不认识。也许他只是长得很像我而已吧。 我走过去向他打招呼，他热情地介绍了自己，用的竟然是我的名字。这很奇怪，但至少证明了我没有产生错觉。我只好信口胡诌一个名字回应他。这些年来我的样子改变了不少，想必是他一时没有认出来吧。 我把箱子寄存给他的同伴，邀他去河边散步。离出发还有一段时间，正好我也好奇，他到底是否意识到了自己的处境。也许我还能给他未来的生活提一两个建议也说不定。 他说自己刚到这里没多久，尽管还有很多东西要适应，但到目前为止都还不错。他交到了新的朋友，也做了一些新的事情，总的来说很愉快，对未来的生活充满信心。 我默默地听着，逐渐感觉到愤恨和妒忌。如果你过着这样的生活，那我这几年又算什么？你怎么敢过得比我还要幸福？ 月亮掩藏在云后，河边的照明也不是很好。我掐住了他的脖子，比我想象中还要柔软。他没什么反抗的机会，一会儿就一动不动了。我将他和他的东西都投到了河里。没过多久，水波平息了。 我在河边散了一会步，又回到咖啡店，准备取回箱子。他的同伴问起他的去向。我说可能是先回去了吧。他们也没再追问下去。 我在地铁站下楼时，一个人乘着自动扶梯，与我擦肩而过。他长得和我以前一模一样，注意力全都在手机上，因此没有看见我。 听说只要一开始愤世嫉俗，就会一直愤世嫉俗下去。我没有管他，只是拎着箱子离开了。","categories":[{"name":"科幻","slug":"科幻","permalink":"https://crvdgc.github.io/categories/科幻/"},{"name":"微小说","slug":"科幻/微小说","permalink":"https://crvdgc.github.io/categories/科幻/微小说/"}],"tags":[{"name":"原创","slug":"原创","permalink":"https://crvdgc.github.io/tags/原创/"},{"name":"日记","slug":"日记","permalink":"https://crvdgc.github.io/tags/日记/"}]},{"title":"什么是 meta 元素","slug":"what-is-meta","date":"2019-07-07T08:56:54.000Z","updated":"2020-11-18T08:55:29.119Z","comments":true,"path":"2019/07/07/what-is-meta/","link":"","permalink":"https://crvdgc.github.io/2019/07/07/what-is-meta/","excerpt":"","text":"引言 meta 来源于 meta fiction，也就是元小说，指小说中的小说。现在的 meta 元素已经被引申为一种泛用的创作手法。因为近年这个概念的火热，接触了很多优秀的 meta 作品，包括以 meta 元素为核心的 Undertale 、 Stanley Parable 、 The Beginner’s Guide ，包含 meta 元素的 Deadpool 、 Shazam! 、 Watchmen 、「一拳超人」，以及传统上不作 meta 解读的「新世纪福音战士」、「路人超能100」、「凉宫春日的忧郁」等等。因此我对这种新的创作手法十分感兴趣，也做了一些（业余性质的）思考。本篇评论将介绍我对 meta 元素的理解，并通过对 Undertale 的分析说明这个概念。关于其他作品的 meta 解读将以这篇为基础，陆续写出来。注意本篇包含了 Undertale 和 Stanley Parable 的剧透。 形式与内容 每件作品，无论是小说、动漫，还是电影、戏剧，都可以看作是作者与读者间一种交流。既然是交流，就需要一些习惯上的约定。这种约定，是因为人与人之间交流的限制才存在的。对于需要传达的内容来说，它们应该是无关的。 比如同一句话可以用不同的语言说出，通常情况下，所传达的含义是不变的。但在某些情况下，我们会有意使其相关。或者说，形式本身会对内容造成影响，当我们刻意强调这种影响，并用它进行表达时，就可以看作是包含了 meta 元素。 就拿语言来说，小说中经常可以见到，夹杂外语这样的方法。比如陀思妥耶夫斯基的「罪与罚」中，夹杂德语单词，表明人物的德国身份；夹杂拉丁文，表明学识广博或着装腔作势；夹杂法语，表示模仿贵族。将外语单词换成对应的俄语单词，看起来不会对内容的传达造成影响，实际上却用说外语这一事实传达了更丰富的含义。 一个更明显的概念是戏剧中，所谓「打破第四面墙」。戏剧中所呈现的房间有三面墙。在戏剧的假设中，观众如同幽灵一样，透过第四面墙进行观察，而对戏剧的内容不造成任何影响。这便是戏剧的语言，或者说戏剧的假设。「打破第四面墙」即与观众产生交互，从而产生了交互戏剧（interactive theatre）。或着甚至直接将观众置身于戏剧当中的沉浸式戏剧（immersive theatre）。通过打破戏剧的假设，或着观众意识到自己在看戏剧，或着剧中人物认识到了自己是戏剧的角色。这种形式本身也开始表达内容了。 meta 游戏 对于戏剧来说，可用于打破的假设其实很少。包括是否察觉戏剧形式的存在，是否察觉观众的存在，是否能与观众进言交流等等。而对游戏来说，所依赖的假设更多，因此打破的方式也就更多了。以下以 Undertale 为例，分析目的不同的多种 meta 方法。 调侃 一种常见的 meta 方法是角色知道自己身处于游戏中。类似的技巧包括小说中人物知道自己身处小说中，情景喜剧人物说，「假如我在情景喜剧中的话我会这么做的」，漫画人物说「你穿得简直像漫画人物」。还有角色对于「主角」、「情节」、「高潮」这样的探讨。这种 meta 程度最低.角色即使说出，也不会当真，仅仅用于向观众传达信息。这种方法常常调侃类型作品中不合理的传统，比如漫画中对人物服饰的夸张。 丰富表达方式 这种 meta 方法作用于场景的层面。常常使用视角、按键、UI等等，用来更有力地表达思想/情感。这种层次上的 meta 元素，如果替换成相应的非 meta 元素，只会削弱表达效果，而不会使表达的内容产生根本上的变化。 视角 玩家通过屏幕观察世界，而且受到虚拟摄像机的限制。通过摄像机的移动和对焦，表示人物的注意。某个场景的设置在画面中产生特殊的视觉效果，而在游戏中的角色不会有这样的感觉。Mettaton在颜色方砖谜题前，说「Didn’t we see this puzzle about a hundred rooms ago?」，指的是他知道玩家每次只能看一个屏幕，因此使用房间数表达进度。这种表达方式削弱了玩家相对于游戏中角色的特权，或着也可以拉近距离。 按键 Undertale 中的青蛙说按「F4」可以切换全屏，但却不知道「F4」的含义。游戏中的角色本不应该思考教程中按键的含义，而角色却开始思考了。这向玩家传达了，这个角色不仅仅是创造出来任意使用的程序，而是有感知有意识的道德主体。 UI控制 与Toriel的战斗中，玩家生命值低下时，魔法攻击会主动避开玩家。决战中，Asgore击碎了Mercy选项，以此表明不能饶恕他。平时战斗中，以攻击强度表示战斗的意愿。三种情况中，人物通过行为进行表达，然而这个行为并非世界内的行为。也即不是在规则下进行表达，而是通过改变规则进行表达，因而可以产生强烈的震撼。 揭示主题 “This is all just a GAME!” - Flowery meta 元素与主题产生共鸣，是最高层的 meta 。这种 meta 元素成为了作品的基础和目的，无法被替换。因而体现了 meta 方法的独特性。 这类作品本身的主题就是对作品形式的探讨。比如 meta 小说中，用小说的形式探讨小说中真实与虚假的关系。 meta 戏剧中，探讨观众与剧中人物的关系，是对戏剧形式的一种评论。 meta 游戏中，则是对游戏这种媒介的探讨。 当然，在这种形式外，也可能有作品能勉强实现，用小说的形式探讨游戏媒介这样的主题，但现实中这种情况几乎不存在。因为探讨的方式强调了媒介的特点，使用同一种媒介不仅能更有力地体现，而且常常是唯一的选择。比如小说中对人物的心理进行深刻地描写，乃至于诞生意识流，或着创造新词的写法。戏剧的特点则是现场性和实时性，只有最近的面向直播（streaming）的游戏才能对标。对于游戏来说，这一无法替代的特点则是交互性。 通过交互性，可以探讨很多问题，比如 Undertale 中对道德选择的探讨， Stanley Parable 对自由意志的探讨。这种探讨只能发生在游戏中，而不能用其他媒介实现。探讨本身也与游戏媒介紧密相关。因此准确来讲， Undertale 中探讨的是游戏中的道德选择， Stanley Parable 探讨的是游戏中的自由意志。 在游戏的假设下，作为世界「主人」的玩家，对世界中的角色，可以进行任何暴力行为，而不用承担任何道德责任。实现这种权力不对等的一个手段是存档（Save/Load，S/L）。 Undertale 中，通过行为的影响无法用 S/L 抹除这一方式，反思了这种游戏假设可能带来的不良影响。 游戏的另一个假设是，玩家可以在一定限度内做出自由选择。而在 Stanley Parable 中，玩家可以通过违反指令做出自由选择，但游戏（叙述者）对玩家违反指令察觉并作出反应这个事实，说明玩家的选择仍然在作者的预料之中，是游戏机制的一部分，而非真正的自由选择。 以上两个例子中，媒介既是表达内容的起点，也是终点。作品通过媒介才能进行表达，而表达出来的内容反而是关于媒介本身的。我认为这是使用 meta 元素的最高层次。","categories":[{"name":"科幻","slug":"科幻","permalink":"https://crvdgc.github.io/categories/科幻/"},{"name":"评论","slug":"科幻/评论","permalink":"https://crvdgc.github.io/categories/科幻/评论/"}],"tags":[{"name":"电影","slug":"电影","permalink":"https://crvdgc.github.io/tags/电影/"},{"name":"游戏","slug":"游戏","permalink":"https://crvdgc.github.io/tags/游戏/"},{"name":"动漫","slug":"动漫","permalink":"https://crvdgc.github.io/tags/动漫/"}]},{"title":"Poorman's PageRank | 从零开始 Haskell 实现 PageRank","slug":"poorman-pagerank","date":"2019-04-26T02:00:20.000Z","updated":"2019-04-26T04:29:19.561Z","comments":true,"path":"2019/04/26/poorman-pagerank/","link":"","permalink":"https://crvdgc.github.io/2019/04/26/poorman-pagerank/","excerpt":"","text":"PageRank 算法是一种经典的网页排名算法。基本思想是，每个节点首先赋相等的初值。接下来，根据链接关系将值传播到链接去的节点。如此迭代直到收敛。 需要特殊处理的地方是，出度为 0 的节点需要将值保存到自己。 为了避免自私的节点不引用别人，从而大量积累自己的值，进行平滑处理。给每一个节点乘以缩减因子 $s$ ，再将每个节点加上相等的 $(1-s)/n$ 。注意到这种平滑不改变总值。也即任何时刻所有节点的值之和恒为 1 。 与之相关的还有 特征向量中心度 eigenvector centrality ，其区别是，不处理出度为 0 的点，也不进行平滑。而在每一步进行正规化。此外，特征向量也可以使用入度作为标准，仅需将连接矩阵转置即可。 这里给出一种简洁的三合一 Haskell 实现。不使用任何复杂的库函数，仅用 80 行。从中可以看到 Haskell 的简洁和抽象能力。 三种算法的核心都是不断迭代直到收敛。将这一逻辑抽象出来得到： 12345converge :: Eq a =&gt; (a -&gt; a) -&gt; a -&gt; aconverge f v = fst $ until theSame update (v, f v) where theSame (x, y) = x == y update (x, y) = (y, f y) 这里用到了库函数 until :: (a -&gt; Bool) -&gt; (a -&gt; a) -&gt; a -&gt; a 。这个函数接收一个判断函数，一个更新函数和初值。当判断函数返回假时，会应用更新函数。当判断函数返回真时，返回最终值。 converge 函数实际上要构造一个流（stream），即 v : f v : f (f v) : f (f (f v)) : ... 。当流的两个连续元素相等时，我们找到了 f 这个函数的不动点，也就是最终的收敛值。 因为只需要比较前两个元素，所以我们使用两个元素的元组（tuple）作为保存的状态。until 的判断函数就是两个元素是否相等。更新函数是抛弃第一个元素，对第二个元素应用 f 。 接下来不同算法的区别，仅在更新函数不同。 对于 pageRank 来说，就是不断乘以连接矩阵： 1234pageRank :: [[Value]] -&gt; [Value] -&gt; [Value]pageRank a vs = head $ converge (`matmul` a') [vs] where a' = compensate a 其中 matmul :: (Num a) =&gt; [[a]] -&gt; [[a]] -&gt; [[a]] 是矩阵乘法，将在下面给出实现。 注意到，首先将初值用列表改成 (n, 1) 的行向量，因此每次迭代改为右乘连接矩阵。最后使用 head 再转变成一维列表 (n,) 。下面各个算法做同样的处理。 compensate 函数实现两个功能，对于出度不为 0 的节点，将因子 1 平均分配到每个非零节点上；对于出度为 0 的节点，将 1 分配到自己的位置上（矩阵对角线）。 123456789101112131415161718compensate :: [[Value]] -&gt; [[Value]]compensate = map procOut . zip [0 ..] where procOut (i, l) = if any (/= 0) l then distribute l else oneAt i l distribute l = let v = 1.0 / (sum l) in map (\\x -&gt; if x == 0 then x else v) l oneAt i l = let (x, _:ys) = splitAt i l in x ++ 1.0 : ys 平滑处理可以改为对连接矩阵进行修改： 12345smooth :: Value -&gt; [[Value]] -&gt; [[Value]]smooth s m = map (map interpolate) m where interpolate a = s * a + (1.0 - s) / fromIntegral n n = length m 对每一个元素，都用因子 s 缩减，再加上补偿。 那么平滑后的 PageRank 算法如下： 1234smoothPageRank :: Value -&gt; [[Value]] -&gt; [Value] -&gt; [Value]smoothPageRank s a vs = head $ converge (`matmul` a') $ [vs] where a' = smooth s . compensate $ a 对于特征向量中心性，需要实现正规化： 1234normalize :: (Fractional a, Ord a) =&gt; [a] -&gt; [a]normalize vs = let m = maximum . (map abs) $ vs in map (/ m) vs 即将一个行向量的每个元素除以最大值。 那么特征向量中心性可以实现如下： 123eiginCentr :: [[Value]] -&gt; [Value] -&gt; [Value]eiginCentr a vs = head $ converge ((map normalize) . (`matmul` a)) [vs] 以上已经实现了三个算法的核心部分。接下来给出辅助函数的直观定义。 矩阵乘法： 12345678dot :: (Num a) =&gt; [a] -&gt; [a] -&gt; adot x y = sum $ zipWith (*) x ymatmul :: (Num a) =&gt; [[a]] -&gt; [[a]] -&gt; [[a]]matmul a b = map rowMul a where b' = transpose b rowMul r = map (dot r) b' 类型转换： 1234type Value = DoubleaFromIntegral :: (Integral a) =&gt; [[a]] -&gt; [[Value]]aFromIntegral = map (map fromIntegral) 生成初始平均分配值： 12normalDist :: Int -&gt; [Value]normalDist n = replicate n $ 1.0 / fromIntegral n 图从边表示转化为连接矩阵表示： 12345678910edgeToAdj :: (Integral a) =&gt; [(a, a)] -&gt; [[a]]edgeToAdj es = [[query i j | j &lt;- [0 .. upper]] | i &lt;- [0 .. upper]] where (ls, rs) = unzip es vs = ls ++ rs upper = maximum vs -- lower bound = 0 query i j = if elem (i, j) es then 1 else 0 其实这里使用 ST monad 更好一点，仅需要 $O(v^2)$ 的时间复杂度。这里用的是直接搜索，需要 $O(v^4)$ 的时间复杂度。 以上代码实现了所有三个算法的功能，仅用了 80 行代码。完整代码见 gist 。 使用下图进行测试： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758-- Test Graph 2tg2e = [ (0, 8) , (1, 6) , (1, 10) , (1, 11) , (2, 1) , (2, 10) , (2, 11) , (3, 15) , (3, 17) , (4, 1) , (4, 6) , (4, 15) , (5, 7) , (5, 8) , (5, 16) , (6, 5) , (6, 8) , (6, 16) , (7, 5) , (7, 13) , (7, 15) , (8, 16) , (8, 5) , (8, 6) , (9, 11) , (9, 10) , (9, 2) , (10, 9) , (10, 11) , (10, 13) , (11, 9) , (11, 10) , (11, 15) , (12, 13) , (12, 15) , (12, 16) , (13, 14) , (13, 15) , (13, 16) , (14, 13) , (14, 12) , (14, 15) , (15, 1) , (15, 9) , (15, 11) , (16, 7) , (16, 8) , (16, 13) ]tg2 = edgeToAdj tg2etg2spr = smoothPageRank 0.8 (aFromIntegral tg2) (normalDist . length $ tg2)printTg2spr :: IO ()printTg2spr = mapM_ (printf \"%.3f\\n\") tg2spr 测试结果如下： 123456789101112131415161718192021222324$ stack ghciλ&gt; :load pagerank.hs[1 of 1] Compiling Main ( pagerank.hs, interpreted )Ok, one module loaded.λ&gt; printTg2spr0.0110.0490.0340.0110.0110.0540.0450.0480.0690.0870.0840.1040.0200.0830.0330.0950.0830.078λ&gt; 符合预期。 连矩阵乘法都从头开始写，到整个算法完成，仅需要 80 行代码。核心就是 converge 函数的抽象。这个例子很好地体现了 Haskell 作为函数式语言的优点。","categories":[{"name":"计算机","slug":"计算机","permalink":"https://crvdgc.github.io/categories/计算机/"},{"name":"函数式程序设计","slug":"计算机/函数式程序设计","permalink":"https://crvdgc.github.io/categories/计算机/函数式程序设计/"}],"tags":[{"name":"haskell","slug":"haskell","permalink":"https://crvdgc.github.io/tags/haskell/"},{"name":"social-network","slug":"social-network","permalink":"https://crvdgc.github.io/tags/social-network/"},{"name":"math","slug":"math","permalink":"https://crvdgc.github.io/tags/math/"}]},{"title":"Poorman's SE VCG | 从零开始 Haskell 实现搜索引擎 VCG","slug":"poorman-vcg","date":"2019-04-25T16:44:53.000Z","updated":"2019-04-26T04:30:08.825Z","comments":true,"path":"2019/04/26/poorman-vcg/","link":"","permalink":"https://crvdgc.github.io/2019/04/26/poorman-vcg/","excerpt":"","text":"VCG 即 Vickrey–Clarke–Groves 机制，是一种具有很多优秀性质的拍卖机制。包括完美匹配、市场出清、在所有可能的清仓价格中总价格最低、总收益达到社会最优、买家按真实估值出价是出价均衡而且是最优策略等等。 搜索引擎广告位拍卖是一种特殊的拍卖形式。详细分析见《网络、群体与市场》 第 15 章。（英文预印本可在 官方网站 合法下载。）假设广告位 $i$ 的点击率 $r_i$ ，广告商 $j$ 对单点击的估值为 $b_j$ 。那么广告商 $j$ 对广告位 $i$ 的总估值 $v_{ij} = r_i b_j$ 。 注意到同一个广告位的点击率对所有广告商相同，因此对单点击估值最高者获得点击率最高广告位，依此类推。这就为搜索引擎中的 VCG 计算带来的简化的空间。 每个广告商所支付的价格等于假设它不出现时，其他广告商所能获得的总福利增加。（也就相当于它出现给其他所有人带来的总损失。） 不妨将广告从点击率高到低排列，广告商按估值从高到低排列。那么估值最高者所支付价格等于第二高的广告商如果取得第一高的广告位能增加的福利（$(r_1 - r_2) b_2$），加上第三高的广告商取得第二高的广告位能增加的福利（$(r_2 - r_3) b_3$）等等。也就是估值矩阵（$v_{ij}$）主对角线减次对角线。 这里给出一种超轻量的 Haskell 实现。 首先对原公式进行变形： $$\\begin{align} p_j &amp;= \\sum^{m}_{i=j+1} b_i(r_{i-1} - r_i) + b_{m+1}r_m \\\\ &amp;= \\sum^{m+1}_{i=j+1} b_i(r_{i-1} - r_i) \\end{align}$$ 其中 $r_{m+1} := 0, b_{m+1}:= \\mathbf{if} \\ m=n\\ \\mathbf{then} \\ 0 \\ \\mathbf{else} \\ b_{m+1}$ 可以将原有的点击率列表和每点击估价列表，通过填充 $0$ ，将边界情况统一成一般情况。 给定某广告位之后的点击率列表和估价列表，计算该广告位价格的方法如下： 12345price :: [Int] -&gt; [Int] -&gt; Intprice rs bs = sum $ zipWith (*) padbs padrs where padbs = tail $ bs ++ [0] padrs = zipWith (-) rs $ tail rs ++ [0] 首先，padbs 根据上面的推导分别在原有列表尾部添 0 。如果广告商不足（$m=n$），则会使用多余的 0 ；如果已经足够（$m&gt;n$），则添加的 0 会在 zipWith 中被舍弃。 填充之后，padbs 进行了 tail ，这是因为每个广告上的价格是靠其后广告商估值计算得到的。 padrs 负责计算当前广告位点击率和下一个广告位点击率之差。 而主函数部分只是一个简单的向量内积。 price 函数整体上实现了，给定某一广告位开始（包括自己）的点击率列表，和估值列表，可以计算这一广告位的价格。 计算所有广告位的价格的方法如下： 12vcg :: [Int] -&gt; [Int] -&gt; [Int]vcg rates bids = init $ zipWith price (tails rates) (tails bids) 这里利用了 tails :: [a] -&gt; [[a]] 函数，需要 import Data.List (tails) ，其作用是接收一个列表，不断去除首元素，再将结果收集成一个列表。 例如： 12λ&gt; tails \"abc\"[\"abc\", \"bc\", \"c\", \"\"] 函数首先生成了每个广告位开始（包括自己）的点击率和估值列表，再将其作为参数传给 price 。最后加 init 是因为 tails 函数会将空列表作为最后一个元素。我们将这个价格抛弃。 测试如下： 1234λ&gt; vcg [5,3,1] [15,8,5][26,10,0]λ&gt; vcg [5,1] [11,7,5,3,2,1][33,5] 实现正确。 有效代码只有 4 行，只用了一个简单的库函数。这个例子很好地体现了 Haskell 的简洁和抽象能力。 注：题图来源 https://commons.wikimedia.org/wiki/File:Tea_auction_australia.jpg ，授权 CC BY 2.0 ，作者 State Library of Victoria Collections","categories":[{"name":"计算机","slug":"计算机","permalink":"https://crvdgc.github.io/categories/计算机/"},{"name":"函数式程序设计","slug":"计算机/函数式程序设计","permalink":"https://crvdgc.github.io/categories/计算机/函数式程序设计/"}],"tags":[{"name":"haskell","slug":"haskell","permalink":"https://crvdgc.github.io/tags/haskell/"},{"name":"social-network","slug":"social-network","permalink":"https://crvdgc.github.io/tags/social-network/"},{"name":"math","slug":"math","permalink":"https://crvdgc.github.io/tags/math/"}]},{"title":"【有剧透】奇异人生 - 人类的能力是有极限的","slug":"life-is-strange","date":"2018-10-27T16:30:19.000Z","updated":"2019-04-29T02:13:48.878Z","comments":true,"path":"2018/10/28/life-is-strange/","link":"","permalink":"https://crvdgc.github.io/2018/10/28/life-is-strange/","excerpt":"","text":"12345\\\\\\ /// 剧透预警/// \\\\\\ 包含「奇异人生」的主要剧透 「奇异人生」对细节的雕琢和对场景的表现十分精彩，结局却没能让所有人满意。无论之前做出了何种选择，最后却归结到简单的二选一上。至少故事是这么展现的，也希望让你相信是这样的。但这个选择真的有意义吗？让我们倾听一下理性之声吧。 选择有意义的前题是，对选择可以造成的后果有比较完全的知识。 让我们先看一个买彩票的例子。 一个数学家去买彩票，买了「00 00 00……」这样的号码。旁边的彩民感觉很奇怪，因为他/她每次购买的都是精心挑选的一系列数字。她/他认为，数学家的选择会造成中奖率降低，因此他/她认为数学家的选择是不对的。 更加理性的数学家会发现，假设彩票的数字真的是随机抽出来的话，那么不同的选择造成的后果其实没有差别。选择「00 00 00……」只是个人偏好，外加节省力气而已。 当然，这么做合理的前题是，彩票真是随机抽出来的。假设彩票公司动了手脚，问题就转化为你对彩票公司抽奖机器的掌握程度了。显然，信息越多，越有助于做出有利的决策。 彩票和奇异人生有什么关系呢？ 还记得结局的两个选择么？要么牺牲 Chloe 保护小镇，要么救下 Chloe 牺牲小镇。但真的是这样么？ 做出选择的依据在哪里？要知道，「蝴蝶效应」类故事的一个关键在于混沌理论，也就是初始条件微小的差异会造成巨大的，难以预计的后果。这种对确定性的破坏，保证了人们难以做出完美的选择，从而形成了故事的主要矛盾。 因此，你的两个选项并非「要么 Chloe ，要么小镇，其他完全相同」（如电车难题一样），而是「 Chloe 外加这一事件的未来，和，小镇外加这一事件未来」。要知道影响可能持续成千上万年，甚至改变整个人类的历史进程。怎么样？有没有感觉肩上的担子稍微沉了一些？ 在如此巨大且不可知的未来中，几条人命又算得了什么呢？但如果不考虑这些的话，我们又能考虑什么呢？无论如何选择都会有风险，微小的差异可能改变一切。如此不可知、多变、又危险的后果，很难想象怎么做出「正确的选择」。 因为要在混沌中正确，就要求近乎「全知」的能力。而人类的能力是有极限的，你不能拯救所有人（某切丝做出了错误示范）。在这种情况下，无论如何选择，很难分辨到底哪种是「道德」的，或者说「正确」的。其实只是一个个人偏好问题。你喜欢人多热闹一点，就牺牲 Chloe 。你喜欢 Chloe 就留下她。 如果有人问，为何你如此自私，牺牲那么多人，就为了一个人？那你可以回答，有可能我救下一个镇子的人，未来人类会死于热核战争。我说不准，你也说不准。所以，其实，我爱怎么样就怎么样吧。 等等，这句话怎么这么像 Rick &amp; Morty 里的台词？没错，由混沌理论带来的不可知论，消灭了选择的意义，也就消灭了道德的意义。本身带有强烈的虚无主义色彩。在这种世界中，意义不会来自于外部。所以，你只能选择要么像 BoJack 一样，沉溺于虚无，要么像 Rick &amp; Morty 或者尼采那样，嘲笑虚无，自己为自己设立意义并乐在其中。 这样的说法也可以说是顺其自然，因为无论你有没有回溯能力，做出的每一个微小的改变都有同样的影响，试图修正是很困难的。 因此，当有人（比如制作组）惊讶于你竟然选择了 Chloe 时，你可以说，「我只不过在彩票上买了一串 0 而已」。 最后补充一点，我知道故事的逻辑和现实的逻辑不一样。故事的世界不遵循理性，因为它的力量正源于此，说让你二选一，其他都一样，就可以做到其他都一样。说白了，故事世界的上帝，也就是制作人，是个混蛋一样的上帝，只有如此，才能让里面的角色展现自身。所以这里只是如果从现实的角度看待会怎样的想法。","categories":[{"name":"科幻","slug":"科幻","permalink":"https://crvdgc.github.io/categories/科幻/"},{"name":"评论","slug":"科幻/评论","permalink":"https://crvdgc.github.io/categories/科幻/评论/"}],"tags":[{"name":"游戏","slug":"游戏","permalink":"https://crvdgc.github.io/tags/游戏/"}]},{"title":"Adorable lores","slug":"adorable-lores","date":"2018-08-11T12:09:09.000Z","updated":"2018-08-11T13:35:50.000Z","comments":true,"path":"2018/08/11/adorable-lores/","link":"","permalink":"https://crvdgc.github.io/2018/08/11/adorable-lores/","excerpt":"","text":"前言 lore 这个词在日常中不大常见[1]。见得更多的，可能是它的亲戚 folklore （民俗）。不过在科幻奇幻世界（universe）构建，以及游戏背景中经常可以看到这个词。它到底是什么意思呢？和一般的 story 有什么区别？二者哪个更好？我们能从已有的例子中学到什么构建世界以及叙述上的技巧呢？ 本篇博文将通过几个例子，尝试回答以上问题。可能是因为在学 GRE 吧，结构写得像中学作文，不过我发现这是最有效的交流方式。 注意，前方将有 黑暗之魂 1 和 3 、 银河帝国系列 的主要剧透（达到影响首次体验程度）。此外，还有 冰与火之歌系列、VA-11 Hall-A: Cyberpunk Bartender Action、 差分机 、 高城堡里的人 的轻微剧透（不多于书的腰封、游戏简介的程度）。 我已经警告过你了 The Core of the Lore 世界不是从一片混沌中忽然出现的。无论事物现在呈现什么状态，它们一定不是亘古不变的，发展成为现在的样子一定有其原因。 ——奥森·斯科特·卡德[2] 从词源上看，lore 本意包含 teaching 。词义是关于某一主题的传统或知识，通过教育或经验，随着时间的累积获得。在幻想世界中，与 backstory 接近，时间上位于现在发生的故事之前。形式上可以是故事，比如某个人物的起源故事（origin story）；或者是某一类人对其他人和物的态度，比如神话等。 从这个角度来看，lore 为当前故事提供了一个语境。单纯的 exposition 只描述了世界现在的状态，而 lore 解释了来源。可以说是在构建世界的硬件背景（地理环境、科幻/奇幻设定）之上的软件背景。不过 lore 本身并不是设定。相比之下，更关注生活在世界中的人们。Lore 体现了人们对世界的认知。Lore 可以说是一种集体无意识产物，神话（myth）是 folklore 中比较重要的一种，不过一般的 lore 不一定要包含“神”，也可以是传奇（legend）。 我们通过 lore 来一窥世界中人们的精神状态。我们关心世界，不过更关心 lore 中的人，以及讲述、学习 lore 的人。我们知道他们对世界的认识，从而了解他们的性格，他们如何行动，他们做出选择的原因，以及每个选项上承受的重量。 不了解 lore ，我们同样可以读故事（story）。毕竟，故事只是时间上连续发生的事件而已。不过 lore 为我们提供了深入理解其情节（plot）的机会。神话是社会向其组成个体解释自身的故事。通过了解神话与 lore ，我们得以聆听这种教诲，甚至成为这一虚拟社会中的一员。 由此， lore 可以大大增强故事的沉浸感。不只是 知道 ，而是 懂得 ，甚至于 进入 。这样的体验，对于科幻和奇幻迷来说，难道还不够激动人心么？ Lore 在小说的应用由来已久。近来，它们在游戏中的作用也逐渐显现。下面将举例说明这两点。这篇博文 将 lore 与传统的故事叙事（里面称作 cinematic story-telling）进行对比，其中的观点在下面也会谈到，推荐阅读。 Explore the Lore 小说 这里统一指小说、电影、漫画等线性文本。其中对 lore 的运用实在是太多也太古老了。即使是现实主义小说，也包含着 lore 成分在其中——只不过不是构建出来的，而是真实世界已经存在的。这样的 lore 自不必说，已经埋藏在文本之中了。而虚构世界也需要虚构的 lore ，运用得当，则作用巨大。 Lore 让故事更加有趣。 银河帝国 系列中，阿西莫夫将神话学与民俗学玩得出神入化，整个小说当做历史小说来读也完全没有问题。不仅每个故事中，众多 lore 交织。更神奇的是，由于小说的时间跨度，前面的人物成为了后面的 lore 。而读者作为“历史的亲历者”，看到故事中的后人对前人世界的敬仰或者误解，不仅享有知道真相的快感，也理解了后人对历史的态度。“你们的生活是一个谎言”，有时会想大声地对人物喊道。不过，谁又能知道过去全部的真相呢？我们也是听着各种各样的 lore 长大的。从这种反思中，我们得以从传统的视角中跳脱出来，不是以怀疑论者的身份，而是一个“趣味猎人”的身份，问出那个问题：“凯撒真的存在过么？” 好的 lore 会增强一致性（consistency），让虚构的故事真实可信。故事本身是关于特定时间、特定地点的一系列事件的。但当一个故事成为了 lore 后，其影响会无处不在。你可以从不同地方看到了故事产生的各种影响——它们或者指向不同的角度，或者揭示了真相。你还可以从中观察到每个人的反应和态度，从而更加清晰地勾勒出了其性格。奇幻小说天然与 lore 相性相合，比如 猎魔人 、 指环王 等。 冰与火之歌 构建世界的方式正是如此。当一个重要事件发生时，各种人，各种事件中，都会体现其影响，就如同真实的世界一样。影响的范围从王公贵族、神职者、骑士、马夫、乃至乞丐、盗贼；从事件的发生地，到遥远的地方；从现在，到多年以后。再加上冰火独特的轮换视角叙事方式。我简直想不明白为什么还没有历史学家的博士论文写维斯特洛。 或然历史（alternative history）类的幻想小说中，两个世界的 lore 相通。我们的历史自动成为他们的历史，而我们的世界却与他们的世界如此不同。正因为我们了解 Ada Lovelace 的故事， 差分机 中的角色才更加有趣。我们知道第三帝国的恐怖政策，因此在 高城堡里的人 中，它们成为现实才显得格外恐怖。 游戏 游戏的核心是交互性（interactivity）。我想你已经发现它与故事（story）的矛盾了。 首先，游戏不是必须有故事。想想 吃豆人 ，人们不想知道，也不需要知道吃豆人为什么要吃豆，幽灵为什么要追吃豆人。人们想要的只是摇杆。 其次，故事反而会伤害交互性。为了达到最佳叙事效果，故事的时间、地点、背景，人物的性格、行为方式都需要精心选择。限制选择，也就限制了交互的自由，也就损害了游戏性。 使命召唤 系列最为评论家厌弃的就是其电影脚本式的 单向线性 结构。没有什么挑战的内容，也就无从发挥创造力，玩家所做的不过是在两段 CG 动画中间，跟着各种箭头和标志，完成最简单的动作，毫无乐趣可言。 与之相对的，最好的游戏是完全的开放世界（open world）游戏么？大家玩过的最自由的开放世界游戏，是现实世界。不过大多数人不会觉得它比某些游戏更具吸引力（否则游戏产业就不复存在了）。可见，如果没有一定的精心的调整与选择，游戏也不会那么好玩。 我认为，这就是 lore 最具优势的地方：达到了故事与交互性的平衡。 Lore 本身是故事，是“不必要的故事”，同时也是“有了更好的故事”。以 黑暗之魂 中的 lore 为例，这个游戏将其发挥到了极致。 “什么？ 黑暗之魂 有剧情吗？”说存在当然是存在的，不过你可以完全不看。出色的交互设计本身就让它成为一款好游戏。相应的，主线故事简单：主角需要完成几项小任务，从而完成一个大任务。 反而是 lore 构成了黑魂真正的剧情。每一件物品都有对应的描述，显然不是给剧中角色看，而是给玩家看的。然而每一件的描述又不那么完整，像是零碎的点。但当你收集到足够多后，就可以像福尔摩斯一样将其联系起来了。合在一起，它们讲述了各种各样的 lore 。 它们是可选的，是的，不过它们也是“有了更好的”。你在打各式各样的龙类生物时，会在武器上抹上黄金松脂，从而造成更大的伤害。不是因为你试了各种松脂，或者查了攻略，而是因为你了解古代的战士，使用雷电的力量进行猎龙的传说。使用雷电，代表了你已经成为了这个世界的一部分。你表达了对世界传统尊敬与传承，而这个世界也因此奖励你——让作战变得更容易了，同时也展现了其内在一致性。 你了解角色的背景，他们的为人行事，会对游戏产生新的认知。当你听到了鲁道斯的痛苦、防火女的折磨、巨人王尤姆的悲剧时，你会对自己的使命产生怀疑，这也许会让你做出不同的选择，也许不会。但不同的是，你已经了解了这个世界，进入了这个世界了。 在这一刻，玩家与角色的割裂不再存在，你不是坐在屏幕前“打怪”、“通关”，而是真正融入到了世界之中，你和你所扮演的角色一样了解世界的过去，世界中的人。而这一切，不是靠 CG 或脚本强加于你的，而是你自己的选择。如此， lore 成为了游戏性与故事性的平衡点，也可以达到理想的角色扮演游戏效果。故事的背景、人物通过 lore 实现，而你在此基础上，理解了你的人物，从而做出选择，完成自己的故事。而不是因为你根本不知为何存在的选项限制，也不是毫无限制的任意开放（如同厨师让你自己决定所有调料用量一样不靠谱）。 此外，探索 lore 的过程本身就是一个小解谜游戏，自有其中的乐趣。黑魂使用 lore 完成的是背景介绍。而 VA-11 Hall-A: Cyberpunk Bartender Action 中，当前时间的叙事也都是靠类似于 lore 的方式得来的。你作为一个酒保，从客人口中，从新闻软件、在线论坛中，看到同一件事的不同侧面，从而像拼图一样，还原出事件的真相，勾勒出了一个完整的、令人信服的赛博朋克世界。 最后，系列小说/游戏中，前作的故事成为了后作的 lore ，可算得上是一个自我指涉的粉丝福利。 More on the Lore 看完了 lore 的作用，那么如何在创作中使用 lore 呢？ 首先，要保证一致性。至少作者心中应当知道事件的真相。如果不同的叙述需要出现矛盾，应当有理有据。如果自己先历史虚无了，那读起来可信度也会下降。 其次，和其他所有设定一样，不要“老教授的演讲”，也就是一大段对设定的解说，两个本身知道这件事的人，非要说一遍，只是为了给读者进行 exposition 。找一个“读者的眼睛”，一个“华生”，新来的人，失忆的人，记者，解释给他/她听。更好的方式是，在叙述中自然带出，“show, do not tell”。 此外，读者应该能通过线索大体还原出来，或者至少形成自己的理论。缺失一大块重要部分不一定能增加神秘感，反而可能让人失望。当然也别太容易。 lore 中的人或事物在故事中出现通常是个好主意。产生一种神话照进现实的超现实效果。 结语 一篇好好的杂谈写得像论文/作文一样。最后闲谈几句。 我特别喜欢有 lore 的作品，如果知道请务必推荐给我。此外，正如中文科幻圈中有用 PKD 作为菲利普·K·迪克的简称一样，我一直致力于推广自己发明的“奥森·斯科特·卡德”的简称——奥斯卡。为了让这条迷因扩散，我一定要在这里提一句。 https://www.merriam-webster.com/dictionary/lore ↩ 奥森·斯科特·卡德. 如何创作科幻小说与奇幻小说[M]. 百花文艺出版社, 2015. ↩","categories":[{"name":"科幻","slug":"科幻","permalink":"https://crvdgc.github.io/categories/科幻/"},{"name":"评论","slug":"科幻/评论","permalink":"https://crvdgc.github.io/categories/科幻/评论/"}],"tags":[{"name":"游戏","slug":"游戏","permalink":"https://crvdgc.github.io/tags/游戏/"},{"name":"创作","slug":"创作","permalink":"https://crvdgc.github.io/tags/创作/"},{"name":"设定","slug":"设定","permalink":"https://crvdgc.github.io/tags/设定/"}]},{"title":"论文笔记： Improving Neural Language Models with a Continuous Cache","slug":"continuous-cache","date":"2018-08-11T12:03:55.000Z","updated":"2018-08-11T13:59:29.000Z","comments":true,"path":"2018/08/11/continuous-cache/","link":"","permalink":"https://crvdgc.github.io/2018/08/11/continuous-cache/","excerpt":"","text":"摘要 块引用代表评论 本文提出了一种类似于 cache 的简化 memory RNN 方法。访问 cache 的方式是隐藏层的点积。和直接使用 memory 的方法相比， cache 无需训练，代价较低，因此空间更大。 前言 序列模型在建立语言模型上的一个问题是无法根据近期历史调整（adapt to recent history）。一个解决思路是 memory ，通常需要学习一个参数化的读写机制（learn a parametrizable mechanism to read or write to memory cells）。高昂的代价限制了其 memory 大小和可使用的数据量。 本文提出的 Neural Cache Model 主要思想和 count-based model （n-gram） 中的 cache 方法类似，可以看作其连续版本。前者的解读请看 Kuhn 的 cache 模型 。 模型 思路 语言学知识表明，一个单词在文档中出现一次过后，更有可能再次出现。因此，除了全局模型以外，cache 储存部分近期历史。使用 cache 得到的结果与全局模型的结果进行差值（interpolate）得到最终结果。 优点有： 语言模型能高效适应新领域 见过 OOV (out-of-vocabulary) 单词一次后，即可预测 通过产生更一致（coherent）的数据，改善文档级别的长依赖识别 数学表示 $$p_{cache}(w|h_{1…t}, x_{1…t}) \\propto \\sum_{i=1}^{t-1} 1_{ {w=x_{i+1}}}\\ \\exp (\\theta h_t^\\mathsf{T} h_i)$$ 也就是，从 cache 中生成单词 $w$ 的概率正比于当前隐藏层状态 $h_t$ 与之前所有生成 $w$ 的隐藏层状态 $h_i$ 的点积相似度的指数之和[1]。 使用类似 attention 的方法，把单项操作转化为多项求和，是离散操作连续化的常用手段。见下方。 可见，隐藏层状态作为 cache 的索引，当前隐藏层与 cache 中隐藏层状态相似，会提高对应单词的产生概率。符合上面的思路，而且更易生成重复模式。 语言模型即为二者插值： $$p(w|h_{1…t}, x_{1…t}) = (1-\\lambda)p_{vocab}(w|h_t) + \\lambda p_{cache}(w|h_{1…t}, x_{1…t})$$ 另一种组合方式是 global normalization ： $$p(w|h_{1…t}, x_{1…t}) \\propto \\left( \\exp(h_t^\\mathsf{T}o_w) + \\sum_{i=1}^{t-1} 1_{ {w=x_{i+1}} }\\ \\exp (\\theta h_t^\\mathsf{T}h_i + \\alpha) \\right)$$ 其中参数 $\\alpha$ 起着和 $\\theta$ 一样控制权重的作用。 实验部分表明，前者效果更好，且更容易实现，因此下面的方法均指插值。 实验 PTB 在 PTB 上的实验表明已经达到 state-of-the-art 。 中等规模数据集 在中等规模数据集上进行实验。数据集使用 wikitext103 和 wikitext2 （训练集规模分别为 103M 和 2M ，测试集相同），还有 text8 （同样从 wikipedia 使用不同预处理得到）。 首先看可扩展性： Fig. 4 展示了和 unigram 加 cache 相比，本文模型效果和扩展性更佳。 Table 2 表明，和 LSTM baseline 相比，本模型在 2M 上提升 30% ，在 103M 上提升 16% 。符合 (Goodman, 2001)[2] 观察到的，在大数据集上，复杂技巧带来的提升会下降。和 Pointer Sentinel LSTM [3] 相比， cache 大小同样为 100 时，在 2M 上效果差不多。但 cache 方法计算代价低，因此很容易更大的 cache 。使用 2000 的 cache 大小相比，效果提升了很多。 另外，注意到用 103M 的 LSTM baseline 已经比 2M 上的复杂方法好很多了，因此认为需要在更大数据集上实验（虽然本文没继续做了）。 一个拿 103M 训出来的 LSTM 已经强过 2M 上比较复杂的模型了。说明数据真实王道。学术上用相同数据量比较，为了公平。实际做出产品的话，没人管公平不公平了。 结论 本文中的模型从技术上虽然属于 memory-augmented RNN ，但其结构可以避免学习内存查找部分（memory lookup component），因此计算代价十分低，也很容易加到现有的模型上。 Sainbayar Sukhbaatar, Szlam Arthur, Jason Weston, and Rob Fergus. End-to-end memory networks. In NIPS, 2015. ↩ Joshua T Goodman. A bit of progress in language modeling. Computer Speech &amp; Language, 2001. ↩ Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. ↩","categories":[{"name":"计算机","slug":"计算机","permalink":"https://crvdgc.github.io/categories/计算机/"},{"name":"人工智能","slug":"计算机/人工智能","permalink":"https://crvdgc.github.io/categories/计算机/人工智能/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"https://crvdgc.github.io/tags/RNN/"},{"name":"cache","slug":"cache","permalink":"https://crvdgc.github.io/tags/cache/"},{"name":"NLP","slug":"NLP","permalink":"https://crvdgc.github.io/tags/NLP/"}]},{"title":"Cache Method in n-gram","slug":"kuhn-cache","date":"2018-08-03T16:55:36.000Z","updated":"2018-08-03T18:02:05.000Z","comments":true,"path":"2018/08/04/kuhn-cache/","link":"","permalink":"https://crvdgc.github.io/2018/08/04/kuhn-cache/","excerpt":"","text":"前言 本篇简略介绍一下 (Kuhn, 1988)[1] 和 (Kuhn, De Mori, 1990)[2] 中的主要思想，即在 n-gram 基础的马尔科夫模型中引入 cache 机制的方法。作为讨论后续使用神经网络方法引入连续 cache 的序列模型的铺垫。 思想 为序列模型引入 cache 是基于这样的假设：近期使用的单词，出现频率比全局更高。这是一个受语言学启发的假设。 原语言模型基于 n-gram ，论文中提出的模型为其增加了 cache 部分。以 Part-of-Speech (POS) 作为线索，改进语言模型。这是基于另一个假设：一个内容词（a content word），比如特定的名词或动词，倾向于集中出现；而功能词（function words），倾向于平均分布。 模型的核心可以用以下公式概括： $$ P(W_i = W | g_i = g_j) \\approx k_{M, j} \\times f(W_i = W | g_i = g_j) + k_{C, j} \\times C_j(W, i) $$ 各项含义如下： $W_i = W$ 即第 $i$ 个位置上的单词是 $W$ $g_i = g_j$ 即第 $i$ 个位置上的 POS 是 $g_j$ $k_{M, j} + k_{C, j} = 1$ ，两项分别代表全局 Markov 模型部分概率权重和 cache 部分概率权重。注意到不同的 POS 对应不同的权重分配。 $f(W_i = W | g_i = g_j)$ 为全局 Markov 模型，这里使用的是 3g-gram 。 $C_j(W, i)$ 为从第 $j$ 个 POS 对应的 cache 中所得到第 $i$ 个位置单词为 $W$ 的概率，代表了语境信息。 进行序列预测时，每个 POS 维护一个 LRU 的 cache 储存一定量的单词。对于第 $i$ 个位置，预测其为单词 $W$ 的概率由前两个位置的 POS 产生各种 POS 的概率，乘以由上面公式计算出不同 POS 生成该单词的概率得到。 以上部分，略去了 trigram 预测 POS 的模型以及对 out-of-vocabulary 单词的处理。具体可见论文。 结果 实验证明了 cache 的有效性，并为内容词与功能词的语言学假设提供了实验支持。实验发现[2]，功能词全局部分比重较大，而内容词 cache 部分与全局部分比重相当。见上方表格 3 。 评论 对语境信息建模的方式很多，直接用 cache 储存起来算是很直接的想法。这篇论文[3]将 Kuhn 两篇中的思想改成了连续版本，依然保留了无需训练的优点。也许有的时候模型偏置（model bias）不需要对处理的过程太过干预，能为其提供获得想要信息的方法就好。 Kuhn, R. (1988). Speech recognition and the frequency of recently used words: A modified markov model for natural language. Proceedings of the 12th conference on Computational linguistics-Volume 1, Association for Computational Linguistics. ↩ Kuhn, R. and R. De Mori (1990). “A cache-based natural language model for speech recognition.” IEEE transactions on pattern analysis and machine intelligence 12(6): 570-583. ↩ ↩ Grave, E., et al. (2016). “Improving neural language models with a continuous cache.” arXiv preprint arXiv:1612.04426. ↩","categories":[{"name":"计算机","slug":"计算机","permalink":"https://crvdgc.github.io/categories/计算机/"},{"name":"人工智能","slug":"计算机/人工智能","permalink":"https://crvdgc.github.io/categories/计算机/人工智能/"}],"tags":[{"name":"cache","slug":"cache","permalink":"https://crvdgc.github.io/tags/cache/"},{"name":"language-model","slug":"language-model","permalink":"https://crvdgc.github.io/tags/language-model/"}]},{"title":"论文解读 Recurrent neural network based language model","slug":"rnnlm","date":"2018-07-31T09:38:18.000Z","updated":"2018-08-01T06:51:36.000Z","comments":true,"path":"2018/07/31/rnnlm/","link":"","permalink":"https://crvdgc.github.io/2018/07/31/rnnlm/","excerpt":"","text":"Annotation: Recurrent neural network based language model 作者 Tomas Mikolov Martin Karafiat Lukas Burget Jan “Honza” Cernock Sanjeev Khudanpur 摘要 块引用表示评论。 本文提出了一个基于 RNN 的语言模型（RNN LM）。实验表明与 backoff 语言模型相比，困惑度（perplexity）可能下降 50% 。 简单直接提出 RNN LM ，使用大量实验证明和 n-gram 相比效果不错（缺点是训练复杂度比较高）。 由于模型比较简单，因此在最后的评论中直接概括一下。这篇论文的引言写得十分精彩，对问题的分析一针见血。（当然说得这么坚定也有实验效果撑着呢，想必下笔的时候也是激动万分。）我十分喜欢，主要呈现一下这部分。 引言 构建语言模型，就是处理序列预测问题（sequential data prediction）。然而，很多自然语言方法都针对于特定的语言领域（very specific for language domain）：假设自然语言可以使用分析树（parse tree）来表示，需要考虑词的形态学（morphology）、语法和语义。即使是基于 n-gram 的最通用的模型，也进行了假设：语言是由原子性的符号（也就是单词）序列（也就是句子）所组成的。句子的结尾起着十分重要且特殊的作用。 特定于语言领域这个观察十分有道理。 n-gram 以句子为单位本身已经带有很强的假设，给予了“句子”一个很高的地位，因此也就无法对句间关系建模。然而衡量语言模型好像没有不用句子假设的，即使是下面提出的 RNN 也是如此。这一段可能是为了反衬 RNN 的泛用性。 对简单的 n-gram 研究到底有没有取得显著进步，值得怀疑。如果从序列预测数据的角度来看，的确取得了很大进步。主要靠 cache models （描述长语境信息）和 class-based models （通过相似词之间共享参数改进短语境的参数估计）。其他进步大多能归结到这两类的效果上。 如果从实际应用的角度来看，那么几乎没有进展。真实世界中的语音识别和机器翻译的系统都是建立在大量的数据上的，一种流行的说法是我们只需要更多的数据就够了。学术界的模型通常很复杂并且仅仅在基于数量十分有限的数据集上效果才好。事实上，大多数的先进技术只比简单的 baseline 提高了一点，且很少在实际中使用。 满满的即视感。不过 RNN 带来的提升的确离现实应用近了一大步。 评论 模型 本篇的模型十分朴素，是一个简单的三层 RNN 。Token 使用的是 one-hot 编码。输入层使用单词编码和隐藏层进行拼接。隐藏层使用 sigmoid 激活函数，输出层使用 softmax 。训练算法是 truncated backpropagation through time ， SGD 。如果没有明显改善，学习率每个 epoch 减半。 Dynamic 模型中一个比较有趣的地方（也是读这篇论文的原因）是使用了 dynamic 的方法。主要区别于传统的 static 方法。Static 指的是模型在训练阶段结束之后，将参数固定，在测试过程中不再改变。Dynamic 方法则是在测试时，利用训练的真实标签继续更新参数。 这种做法的一个结果是不再显式地区分训练集与测试集，因为所有的数据都只处理一次。 (Graves, 2013)[1] 中指出了 dynamic evaluation 比本篇论文报告的效果更好。 作者指出，效果和 cache 类似，但由于其在连续空间中学习，如果两个词之间联系比较紧密，那么测试数据中一个单词的频繁出现也会提高另一个单词出现概率。 另一篇专注研究 dynamic evaluation 的论文解读请看 。 全文 作者认为 RNN 相比于 Bengio [3][2] 中的 FNN 的主要优势在于没有指定固定的语境，而是使用隐藏层的状态概括之前所有的语境信息。优点包括需要指定的超参数数量少，通用性强。缺点是难以捕捉长依赖问题，早在 1994 年的 [6][3] 中就已经指出了。解读请看这篇博客。 本篇将 RNN LM 引入 NLP ，使用的是最朴素的模型（本文发表于 2010 年）。实验发现其效果远好于（各种） n-gram 。（从之后的发展来看，几乎将 n-gram 送入历史的废纸堆了）。这一巨大的提升，打破了语言模型是关于各种 n-gram 以及只要有大量的数据就可以提升效果的神话。（结果现在出现了各种复杂的神经网络模型，以及只要有大量数据就可以提升效果的神话x） Graves, Alex. “Generating sequences with recurrent neural networks.” arXiv preprint arXiv:1308.0850 (2013). ↩ Yoshua Bengio, Rejean Ducharme and Pascal Vincent. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137-1155 ↩ Yoshua Bengio and Patrice Simard and Paolo Frasconi. Learning Long-Term Dependencies with Gradient Descent is Difficult. IEEE Transactions on Neural Networks, 5, 157-166. ↩","categories":[{"name":"计算机","slug":"计算机","permalink":"https://crvdgc.github.io/categories/计算机/"},{"name":"人工智能","slug":"计算机/人工智能","permalink":"https://crvdgc.github.io/categories/计算机/人工智能/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"https://crvdgc.github.io/tags/RNN/"},{"name":"NLP","slug":"NLP","permalink":"https://crvdgc.github.io/tags/NLP/"}]},{"title":"论文解读 Learning Long-Term Dependencies with Gradient Descent is Difficult","slug":"learning-long-term","date":"2018-07-30T09:34:49.000Z","updated":"2018-07-30T10:43:46.000Z","comments":true,"path":"2018/07/30/learning-long-term/","link":"","permalink":"https://crvdgc.github.io/2018/07/30/learning-long-term/","excerpt":"","text":"作者 Yoshua Bengio, Patrice Simard, and Paolo Frasconi 以下介绍中，块引用代表评论。 摘要 指出了 RNN 所面临的问题： temporal contingencies present in the input/output sequences span intervals ，也就是所谓的长依赖问题（long-term dependencies）。接下来指出问题的原因是基于梯度的训练方法。这种方法中存在 trade-off bbetween efficient learning by gradient descent and latching on information for long periods 。 基于此提出的解决方法是使用 alternatives to standard gradient descent ，也就是标准梯度下降外的替代品。 即使作为反向传播算法的提出者， Geoffrey Hinton 在 2017 年也对该算法提出了怀疑。不过近期又发了一篇 Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures ，在一些需要特殊网络结构的数据集上比较了生物学启发的多种训练算法，结果发现效果还是 BP 不好。这篇 1994 年的文章讲了 BP 如何不适合解决序列问题中的长依赖。值得一读。 引言 序列任务中需要系统能够存储、更新信息。从过去输入中计算得到的信息，对于输出是很有用的。 RNN 很适合这样的任务，因为其有内部状态（internal state）可以表示这样的上下文信息（context information）。这种特性来自于结构上的“循环”，静态的神经网络，即使引入延迟（Time Delay Neural Networks）也只能将信息储存特定时间长度，而不能储存不定时间长度。 RNN 的训练算法基于损失函数的梯度下降。比如 back-propagation through time (BPTT) 算法。 Forward propagation (FP) 算法计算代价更高，但可以在线学习。另一个训练受限 RNN 的算法中，动态神经元（dynamic neurons，只有向自己的一个反馈）和 FP 一样在时间上只需要本地信息，但权重更新所需计算只正比于权值数（和 BP 一样）。但其对于一般序列的存储能力有限，因此限制了其表示能力。 Long-term dependencies 的定义是在时间 t 的输出依赖于一个更早时间的 $\\tau \\ll t$ 。尽管 RNN 的表现超过很多统计网络，但更难训练到最优。局部最优的原因是次优解将短期依赖纳入了考虑，但没有考虑长期依赖。 Mozer [19] 发现反向传播很难发现长时间间隔的随机事件。本文从理论和实验上解释这个问题。 很惭愧只学过 BPTT ，对于另外两个都没有听说过。 一个含参数的动力系统（parametric dynamic system）的三个基本要求如下： 能够将信息储存任意时长 能够对抗噪声 参数可训练（训练时间合理） 定义信息锁存（information latching）为一个动力系统将特定比特的信息在状态变量中的长期存储。使用 hyperbolic attractor 的形式化定义将在 4.1 节给出。 hyperbolic attractor 本身的定义也将在第 4 节给出 文章共 5 节。第 2 节提出一个只有满足上述条件的系统才能解决的最小任务。接下来展示一个 RNN 解法，和一个负实验结果，表明梯度下降连这个简单任务都不适合。第 4 节的理论结果解释了一个系统要么稳定能够抵抗噪音，要么能使用梯度下降法高效训练，但不能同时达到。否则，在时间 t 的状态对于时间 0 的状态的导数将随 t 增大而指数减小。 因此，反向传播（以及一般的梯度下降算法）对于长序列效率低，因此不满足条件 3 。第 5 节提出了新的算法，并将其与反向传播的变体和模拟退火（simulated annealing）进行比较。使用的是输入输出依赖可以控制的简单任务。 第 2 节 说明问题的最小任务 该任务是一个序列二分类问题，最终的类别只取决于前 L 个输入值。 也即类别 $\\mathcal{C}(u_1,\\dots,u_T) = \\mathcal{C}(u_1,\\dots,u_L)$ 。而整个输入序列可以具有任意长度 $T \\gg L$ 。 该任务中，长度 L 之后的输入都是不相关的噪声。因此，模型需要有效地储存信息才能解决这个问题。本次实验中， L 之后的输入是均值为 0 的高斯噪声。 上面提到第 3 个标准是可学习性，这里有两个方面：第一，处理前 L 步的输入并正确分类。第二，将信息储存在状态变量中任意时长。在这个任务中，前面的分类和后面储存的时长无关，因此一个简单的解决方法是使用一个锁存子系统（latching subsystem），接收前面分类子系统的信息作为输入。 由于我们希望结果不与特定的分类任务相关（也就是与分类子系统相独立），因此我们只关注后面的锁存系统。一个锁存系统想要处理任意输入序列，就需要能将错误传播回前面的系统，并检测到引发锁存的事件。（propagate error information to a module that feeds the latching subsystem and detects the events leading to latching） 因此，我们将上面的任务修改如下：前 L 个输入可供输入算法调参（can be tuned by the learning algorithm）， L 步之后的输入是随机的高斯噪声。目标函数是二分类（期望输出值分别是 ±0.8）的平方误差和。 经过改造后， $h_t\\ (t=1,\\dots, L)$ 代表了类别信息的计算结果。直接学习 $h_t$ 比从输入中学习参数 $\\theta$ 容易。而且如果 $h_t$ 是对应时间步的输入 $u_t$ 的带参函数的话，也即 $h_t(u_t, \\theta)$ ，代价函数对于 $h_t$ 的导数是一样的（BPTT 下）。如果因为梯度消失导致 $h_t$ 不能被训练出来的话，那作为带参函数同样训练不出来。 研究锁存子系统的方法十分巧妙。锁存子系统想要达到预期的功能，至少要能将分类结果的正误信息传播回分类子系统。也即假设最后是 1 类，至少应该有方法将其通知到分类子系统。而我们观察其能否通知的方式，就是让其在前 L 步输入处还原最终的分类结果。对于原来的任务来说，我们通过训练参数让这个结果最终在 L 步处出现。而训练的依据就是在第 L 步我们接受到了锁存子系统传播回来的正确类别的信息。 举一个例子来说明。假设原来的任务 L = 3 ，接受到一个序列 101001… ，假设标签为第 1 类。分类系统应该在接受到 101 这三个时间步时就已经得出分类标签为 1 这个结论了。接下来，锁存系统将 1 这个结果储存起来，直至第 T 步输出。 然而我们只希望关注锁存子系统。在原任务中，如果最终标签是 1 ，锁存子系统应该能反向传播到第 3 个时间步，告诉分类子系统标签是 1 。因此，假如序列最终的标签是 1 ，我们希望无论前 3 个输入是什么，都能得到这一标签。所以锁存子系统只需要把真实标签传播给前 3 步的分类系统即可。假如没有分类系统，我们就让锁存子系统在前 3 个位置还原最终的标签。 也就是，假设序列的最终标签是 1 （对应目标值 0.8），锁存子系统应该在前 3 步输出 0.8, 0.8, 0.8 ，如果序列最终标签是 0 ，锁存子系统应该在前 3 步输出 -0.8, -0.8, -0.8 。假如我们有真的分类子系统，它应当能在这里拿到真实的标签，并据此将其与输入序列联系起来（通过调整带参函数的参数），比如 101 或者 010 等等。 个人感觉上面的切片测试方法不仅适合测试锁存，应该适合各种具有确定期望功能的子系统。举例来说，假设一个子系统需要进行 (+1) 这个运算，应当有能力对于输出 x 在输入处还原期望的输入 (x-1) ，这样才能通知前面的系统，我需要 (x-1) 。 当然这只是完成锁存或其他功能的必要条件。 关于导数相同一段，这是因为假设 h_t 仅取决于 u_t （而非其他时间步的输入），因此 h_t(u_t, \\theta) 是一个 context-free 的函数，根据求导的链式法则，对 h_t 的导数相同，而不论它是变量还是另一个变量的函数。 关于为何选取目标函数值为 0.8 ，因为 tanh 函数值域为 (-1, 1) ，而下一层的输入在 tanh(-1) = -0.76 到 tanh(1) = 0.76 之间。因此多个 tanh 单元叠加值域就在 (-0.8, 0.8) 之间。 第 3 节 一个简单的 RNN 解决方案 见图 Fig. 1a. 这是一个单一神经元的 RNN ，如果 $w \\gt 1$ ，有两个吸引子 $\\pm \\bar{x}$ ，值取决于 $w$ 。 假设初值 $x_0 = -\\bar{x}$ ，文献 [7, 8] 表明存在 $h^\\star \\gt 0$ 满足： $x_t$ maintains its sign if $|h_t| \\lt h^\\star$ ，也即小于阈值的输入不会改变状态的符号。 there exists a finite number of steps $L_1$ such that $x_{L_1} \\gt \\bar{x}$ if $h_t \\gt h^\\star \\ \\forall t \\le L_1$ 。也即假如超过正向阈值的输入持续了超过 $L_1$ 步后，会在 $L_1$ 步时将状态转到正向吸引子 $\\bar{x}$ 。 对于初值为正的情况有相应的对称结论。 当 $w$ 固定， $L_1$ 随着 $|h_t|$ 的增加而减小。 据此我们可以得到： 该系统可以储存一个 bit 的信息。通过最终输出的符号确定。 存储是通过将足够强（大于 $|h^\\star|$）的输入保持足够长的时间实现的。 小的（小于 $|h^\\star|$）噪声不会影响输出的符号。无论持续时间有多长。 参数 $w$ 也是可训练的，更大的 $w$ 对应于更大的阈值 $h^\\star$ ，对抗噪声的能力也就越强。 比如可以通过调整，使得 $h^1_t \\ge H$ 且 $h^0_t \\le H$ ，其中 $H \\gt h^\\star$ 来实现。 从上面的 Fig. 1b. 我们可以看到成功学习出来了加粗部分的前 3 个 $h_t$ 。 下面看各参数的影响。 首先 Fig. 2a 是噪声的标准差 $s$ 和初始的权值 $w_0$ 的影响。我们可以看到随着 $s$ 的增大和 $w_0$ 的减小，效果变差。 这很符合我们的直觉，噪声越强，对抗噪声的阈值越低，越容易丢失存储。 Fig. 2b 展示了随着 $T$ 的增加，收敛性变差。这表明，梯度下降即使对于长时间存储 1 位信息也很困难。 第 4 节 使用动力系统学习锁存 本节以基于动力系统的实时识别器为例子，说明 RNN 能够按双曲吸引子（hyperbolic attractors）的方式鲁棒性地储存信息的条件，会导致梯度消失的问题。 非自动（non-autonomous）的离散时间的系统，带有额外的输入： $$ a_t = M(a_{t-1}) + u_t $$ 和自动系统（autonomous system）： $$ a_t = M(a_{t-1})$$ 其中， $a_t$ 代表系统状态， $u_{t}$ 代表输入。两者是 $n$ 维向量， $M$ 代表非线性映射。 不带有额外输入的自动系统，可以通过引入额外状态变量和对应输入的方式，转变成非自动的系统。 比如 $a_t = N(a_{t-1}, u_{t-1})$ （ $a_t$ ， $u_t$ 分别为 $n$ 维和 $m$ 维向量）可以转化为 $a_t^\\prime = N\\prime(a_{t-1}\\prime) + u_t^\\prime$ ，其中 $a_{t}^\\prime = (a_t, y_t)$ 是一个 $n+m$ 维向量， $u_t^\\prime = (0, u_t)$ 即前 $n$ 个分量为 0 ，$N\\prime(a_t\\prime) = (N_t(a_{t-1}, y_{t-1}), 0)$ 即后 $m$ 个分量为 0 。 最终，$y_t = u_t$ 。 以上转换相当于将本来的内部状态变量当做系统的额外输入。使用映射计算出下面的 n 维状态后，就将剩下的 m 维分量丢弃。再从外界输入同样的 m 维分量，组合在一起恢复内部状态，作为下一次映射的输入。 注意到具有 $N^\\prime$ 形式的非自动系统也可以等价转换为自动系统。因此，不失一般性，我们只考虑非自动系统。 下面说明，当使用双曲吸引子进行锁存时，只有两种情况会发生：要么对噪声十分敏感，要么代价函数在 t 时刻对于 0 时刻的导数，将随 t 增加而指数下降。 4.1 分析 为了锁存一位信息，希望将系统的活动 $a_t$ 限制在定义域的一个子集 $S$ 上。这样能区分两个状态：在 $S$ 内，和不在 $S$ 内。为了使 $a_t$ 保持在其中，动力系统可以将其放在一个吸引子的吸引盆（basin of attraction）中。（吸引子也可以是子流形或子空间的吸引子）。想要“擦除”这一位信息，系统将 $a_t$ 从吸引盆中推出，可能放进另一个吸引盆中。本节说明，如果吸引盆是双曲的（hyperbolic），或者可以转化为双曲的（比如周期稳定吸引子 periodic stable attractor），那么对 $t_0$ 输入的导数会迅速消失。 定义 1 ： $E$ 是映射 $M$ 的不动点，如果 $E = M(E)$ 定义 2 ： 不动点点集 $X$ 是可微映射 $M$ 的双曲吸引子，如果 $\\forall a \\in X, \\ M^\\prime(a)$ 的特征值的绝对值小于 1 。 吸引子可能包含一个点（固定点吸引子， fixed point attractor），有限个点（周期性吸引子， periodic attractor）或者无限个点（混沌吸引子， chaotic attractor）。 一个稳定的固定点吸引子，对于映射 $M$ 是双曲的；一个稳定的周期性的吸引子，设其周期为 $l$ ，则对于映射 $M^l$ 是双曲的。 RNN 的吸引子的种类取决于权值矩阵。对于 $a_t = W\\ \\tanh(a_{t-1})+u_t$ ，如果 $W$ 是对称的，且其最小特征值大于 -1 的话，那么其所有吸引子都是固定点。如果行列式小于 1 或者系统是线性且稳定的，那么只有在原点处有一个固定点吸引子。 以上关于吸引子的知识全没有接触过。翻译了一下。仅从直观上进行理解。 定义 3 ： 一个吸引子 $X$ 的吸引盆 $\\beta(X)$ ，指映射 $M$ 下收敛于 $X$ 的点集。即 $\\beta(x) = { a\\ :\\ \\forall \\epsilon, \\exists l, \\exists x \\in X \\text{ s.t. } ||M^l(a)|| \\lt \\epsilon}$ 定义 4 ： $\\Gamma(X)$ 是双曲吸引子 $X$ 吸引盆中的 reduced attracting set，如果满足 $\\forall l \\ge 1$ ， $(Ml)\\prime(y)$ 的所有特征值小于 1。 根据定义有， $X \\subset \\Gamma(X) \\subset \\beta(X)$ 。 reduced 应该翻译成“剩余”还是“减小”？不太确定。这里只要求特征值小于 1 ，双曲吸引子要求特征绝对值小于 1 ，故双曲吸引子是 Gamma(X) 的子集。直觉上，特征值小于 1 可能对应上面“将其保持在吸引盆”中的要求。 定义 5 ： 一个系统可以鲁棒性地在 $t_0$ 锁存若干双曲吸引子中的一个吸引子 $X$ ，如果 $a_{t_0}$ 在 $X$ 对于定义自动系统的映射 $M$ 的 reduced attracting set 中。 对于非自动动力系统，只需 $a_t \\in \\Gamma(X) \\text{ for } t \\gt t_0$ 。接下来证明为什么使用 $\\Gamma(X)$ 来储存具有鲁棒性。 定理 1-3 及证明请查阅原文。最终证明了如果在 beta(X) 中，代表不确定性的球体会越来越大，因此输入的微小扰动可能将轨迹引导进入一个错误的吸引盆，即系统无法对抗噪声。相反，如果在 Gamma(X) 中，则能在输入中找到一个界，保证 a_t 一直在 X 中的点的特定距离内。因此是鲁棒的。见 Fig. 3 。 定理 4 ： 当输入 $u_t$ 使得系统在时间 0 后保持在 $X$ 上鲁棒时，随着 $t$ 趋近于无穷，$a_t$ 对 $a_0$ 的偏导趋近于 0 。 也就是说对抗噪声的代价是对过去事件的导数与近期事件相比会小很多。 4.2 对权重梯度的影响 $$\\frac{\\partial C_t}{\\partial W} = \\sum_{\\tau \\le t} \\frac{\\partial C_t}{\\partial a_\\tau}\\frac{\\partial a_\\tau}{\\partial W} = \\sum_{\\tau \\le t} \\frac{\\partial C_t}{\\partial a_t}\\frac{\\partial a_t}{\\partial a_\\tau} \\frac{\\partial a_\\tau}{\\partial W}$$ 因此，相对于较近的事件，$\\tau \\ll t$ 的前两项的乘积较小，因此对最终的结果影响较小。也就是，即使可能存在一个 $W$ 使得 $a_\\tau$ 进入一个更好的吸引盆，但对 $W$ 的梯度不会反映这种可能性。 举例来说，假设 A ， B 两个系统顺次相接完成一项任务。且要求 B 使用 A 在 0 时刻检测到事件的信息，在遥远的 T 时刻使用该信息计算错误。（第 2 节定义的任务符合这个特征）。如果 B 训练不足，不能将 A 的结果锁存，那么 T 时刻的错误对 A 在 0 时刻产生的结果影响非常小。相反，如果 B 能够将信息储存很长时间，正确的梯度会被传播回去，但却迅速消失成为小值。因此， A 很难训练。 第 5 节 替代的方法 本节中给出了模拟退火等算法作为梯度下降的替代算法并在多个任务上测试了结果。每个任务上都有算法比反向传播更佳。 第 6 节 结论 一个未进行讨论的点是类似的问题是否会在混沌吸引子中出现。 这个问题可能也会在深度前馈神经网络（feedforward network）中出现，因为 RNN 按时间展开就是一个共享权值的前馈神经网络。 本文研究并不意味着不能为特定任务训练神经网络，相反，如果有先验知识可以设置神经网络的权值共享和初值，利用起来会提升效果。比如在 latch problem 和 parity problem 中，先使用短序列进行训练可以让系统迅速进入正确的区域。","categories":[{"name":"计算机","slug":"计算机","permalink":"https://crvdgc.github.io/categories/计算机/"},{"name":"人工智能","slug":"计算机/人工智能","permalink":"https://crvdgc.github.io/categories/计算机/人工智能/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"https://crvdgc.github.io/tags/RNN/"},{"name":"长依赖","slug":"长依赖","permalink":"https://crvdgc.github.io/tags/长依赖/"}]},{"title":"编程解决《去月球》 To the Moon 记忆碎片小游戏","slug":"memento-solver","date":"2018-07-28T16:24:21.000Z","updated":"2018-07-30T07:20:16.000Z","comments":true,"path":"2018/07/29/memento-solver/","link":"","permalink":"https://crvdgc.github.io/2018/07/29/memento-solver/","excerpt":"","text":"引言 记忆碎片（Memento）是游戏《去月球》（To the Moon）中的一个解谜小游戏。基本规则已在游戏中介绍。即通过点击按钮翻转某一行、列或者对角线，直至所有碎片都转成同一面。 这个解谜与成就/剧情分支无关，而且谜题设置比较简单，比较适合自己玩。谜题也非随机生成，答案都可以在网上找到。 兴趣使然地写了一个编程通用解法。在此介绍一下。 分析 注意到同一位置进行任意偶数次变换都等价于不变，而奇数次变换等价于进行 1 次。为使总变换次数最少，只需决定每个位置是否进行变换即可。 其次，多个不同变换之间满足交换律和结合律，即任意改变变换之间的顺序不会影响最终的结果。 因此，每个解决方案，对应到将变换集合映射到 {进行，不进行} 集合的一个函数映射。若有 N 种可能的变换，则共有 2^N 种可能的解决方案。 可以使用二进制编码，宽度为 行数 + 列数 + 1 。每一位都对应着翻转行、列或对角线。该位为 1 代表进行变换，该位为 0 代表不进行变换。 接下来我们只需要枚举所有可能的解决方案，并将其中正确的挑选出来即可。 游戏中给出了最佳步数，因此可以作为加速搜索的方式，一旦解决方案中，变换的次数超过最佳步数，则跳过。这样，整个搜索空间大小从 2^N 减小到了 C(N, B) ，其中 B 是最佳变换次数， C 是组合数。 实现 首先处理输入，我们将输入的图形转换为一个布尔矩阵。 12345rowNum = int(input('Row number: '))colNum = int(input('Col number: '))best = int(input('Best move: '))print(\"Input table, 1 for solved, 0 for unsolved\")table = [list(map(lambda x: True if int(x) == 1 else False, input().split())) for i in range(rowNum)] 接下来定义一些工具函数： 12345def check(): return all([all(row) for row in table])def flip(i, j): table[i][j] = False if table[i][j] else True 我们需要从解决方案中提取对应的行、列、对角线进行翻转： 12345678910111213def apply_solution(solution): for r in range(rowNum): if solution[r]: for c in range(colNum): flip(r, c) for c in range(colNum): if solution[rowNum+c]: for r in range(rowNum): flip(r, c) if solution[-1]: # diagnol, from left-bottom for i in range(min(rowNum, colNum)): flip(rowNum-1-i, i) 注意，对角线是从左下角开始的。 类似的，我们定义打印解决方案的函数： 123456789def print_solution(solution): for r in range(rowNum): if solution[r]: print('r%s' % r) for c in range(colNum): if solution[rowNum+c]: print('c%s' % c) if solution[-1]: print('d') 注意，编号从 0 开始，方向是从上到下、从左到右。 d 代表对角线。 主要过程是枚举整个解决方案空间： 1234567891011for solution in itertools.product([False, True], repeat=rowNum+colNum+1): if solution.count(True) &gt; best: continue apply_solution(solution) if check(): print_solution(solution) break else: apply_solution(solution)else: print('No answer for best move %s' % best) 首先，使用 itertools.product 产生所有的编码。 对于每一个编码，如果变换数大于最佳，则跳过当前。 如果找到了一个解，则打印并跳出，否则重新调用 apply_solution 函数将表格恢复原状态。 测试 123456789101112131415161718192021Row number: 3Col number: 3Best move: 2Input table, 1 for solved, 0 for unsolved1 0 01 0 01 0 0c1c2Row number: 3Col number: 3Best move: 3Input table, 1 for solved, 0 for unsolved1 0 00 0 00 0 1r1c1d 附注 itertools.product 该函数产生一系列 iterable 的笛卡尔积（Cartesian product），如果指明了 repeat 关键字参数，则会将前面所有的 iterable 再和自己进行笛卡尔积。 全局依赖 由于对角线的存在，每一次枚举行/列无法完全确定其他某个变换是否进行，因此没有进一步减小空间的机会。 但事实上，除了第一个谜题，游戏中每个谜题都包含对角线。因此，可以默认只在奇数编码中搜索。将对角线放在最后一位也有利于快速找到解决方案。 完整代码 gist 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import itertoolsdef solve_memento(): rowNum = int(input('Row number: ')) colNum = int(input('Col number: ')) best = int(input('Best move: ')) print(\"Input table, 1 for solved, 0 for unsolved\") table = [list(map(lambda x: True if int(x) == 1 else False, input().split())) for i in range(rowNum)] def check(): return all([all(row) for row in table]) def flip(i, j): table[i][j] = False if table[i][j] else True def apply_solution(solution): for r in range(rowNum): if solution[r]: for c in range(colNum): flip(r, c) for c in range(colNum): if solution[rowNum+c]: for r in range(rowNum): flip(r, c) if solution[-1]: # diagnol, from left-bottom for i in range(min(rowNum, colNum)): flip(rowNum-1-i, i) def print_solution(solution): for r in range(rowNum): if solution[r]: print('r%s' % r) for c in range(colNum): if solution[rowNum+c]: print('c%s' % c) if solution[-1]: print('d') for solution in itertools.product([False, True], repeat=rowNum+colNum+1): if solution.count(True) &gt; best: continue apply_solution(solution) if check(): print_solution(solution) break else: apply_solution(solution) else: print('No answer for best move %s' % best)solve_memento()","categories":[{"name":"计算机","slug":"计算机","permalink":"https://crvdgc.github.io/categories/计算机/"},{"name":"杂谈","slug":"计算机/杂谈","permalink":"https://crvdgc.github.io/categories/计算机/杂谈/"}],"tags":[{"name":"游戏","slug":"游戏","permalink":"https://crvdgc.github.io/tags/游戏/"},{"name":"python","slug":"python","permalink":"https://crvdgc.github.io/tags/python/"},{"name":"兴趣","slug":"兴趣","permalink":"https://crvdgc.github.io/tags/兴趣/"}]},{"title":"PKD 电子梦评论 - Real Life / Exhibit Piece","slug":"electricDreams-01","date":"2018-07-25T06:22:46.000Z","updated":"2018-07-30T07:06:43.000Z","comments":true,"path":"2018/07/25/electricDreams-01/","link":"","permalink":"https://crvdgc.github.io/2018/07/25/electricDreams-01/","excerpt":"","text":"电子梦系列 PKD 的作品，改编成为电视剧、电影的不在少数。但我还是对《电子梦》抱有独特期待的。不同于以往短篇改电影（《少数派报告》），长篇改电影（《银翼杀手》），长篇改电视剧（《高堡奇人》），《电子梦》将 PKD 的短篇改编成单元剧。每一篇小说独立，每一集剧也独立。短篇小说是最能体现科幻“点子小说”特点的载体。没有了人物和情节的负担，小说可以尽情发挥某个点子设定。身为 PKD 迷，怎么能错过这部剧呢？ 不过看了剧之后，感觉有几集水平真心一般。故从攒着买全集的钱里拨款，买了一本对应小说集，看看原作是否如此。本系列博客对应于每一集每一篇的评论。 &lt;&lt;&lt;&lt; 主要剧透预警 &gt;&gt;&gt;&gt; Quick Recap Real Life 剧集的主角有两位，一位是 Sarah ，未来世界的女同警察，一次针对警察的屠杀的幸存者，另一位是 George ，生活在现代的布鲁斯韦恩——拥有巨额财产却热衷于亲自行侠仗义打击犯罪。准确来讲，两个人共享思想和记忆，因此只是一个人的两个身份——问题在于，哪个身份是真的？ 两人分别在自己的世界中，通过 VR 设备进入到另一个世界和身份中。他们的爱人 Katie 名字相同，长相也相同，他们都在追查同一个姓名的罪犯。甚至来到同一家餐厅吃着同样的汉堡。区别在于，未来世界的生活“too good to be true”。在现代世界中， George 的妻子被罪犯报复残忍杀害，追捕罪犯也以失败告终。未来世界中， Sarah 不仅家庭美满，也成功抓到了罪犯。 两个 VR 项目都是捕捉潜意识中的愿望，将其转化为体验的。一个世界是以另一个世界为基础想象出来的。 如果 Sarah 的身份是虚假的，那么这个未来世界仅仅是一个“科幻世界”，里面有飞车、女同、幸福的家庭、成功抓捕到了罪犯，一切都是为了缓解丧妻之痛的幻想。 如果 George 的生活是虚假的，现实世界是 Sarah 的赎罪场，用自己的幻想折磨自己，以此发泄自己不配有这样幸福生活的自卑和愧疚——幸存者常有的心理障碍 survivor’s guilt，凭什么我能活下来。 对于 Sarah 的过去有清晰的记忆，而 George 的过去则需经人提醒才能慢慢想起。最终，他/她选择了 George 的生活，打碎了 VR 设备。未来世界的妻子看着自己的爱人迷失在 VR 世界中——原来这个世界才是真实的。 “为什么她选择了那样的人生？” “因为她想被自己的罪过惩罚，无论是真实的，还是虚构的。” Exhibit Piece 根据 wiki 中的链接，本篇小说已经进入公共领域了。这里可以找到电子版和PDF版。 小说的主角 George Miller 是未来高压社会下的一名历史研究员，专门研究 20 世纪 50 年代的历史（本篇小说最初发表时间 1954 年）。为了研究的氛围，他平时身着那个年代的服装（在未来则属于奇装异服）。在一次和上司的冲突后， 他被展品——一个 50 年代生活的复制品——中的声音所吸引而进入了展品中。在那里，他回忆起了自己在其中的生活，一个妻子，两个孩子，有些令人担心的上司。 他感到困惑，因此找到了过去世界中的心理医生 Grunberg ，对方认为两套理论同样有说服力，但 George 却提出了第三个理论——两个世界都是真实的。医生建议他回到自己在世界中移动的点上，这样就能确定哪个世界是真实的。 他在那里见到了自己未来世界的同事和上司，他们认定 George 患了狂想症，威胁他要将他安乐死，并将展品拆除。 George 认为未来的高压社会没有什么值得生活的。与之相比，50 年代竟然成为了自由的黄金年代。 George 表示，展品实际上是一个时空门，他通过它回到了过去，而其他人无法回去。拆除展品只是毁坏了时空门，他将永远留在过去——正是他所希望的。 当 George 回到家中拿起报纸时，却发现上面写着，俄罗斯展示钴炸弹，全世界将面临毁灭。 究竟未来世界是他的幻想，还是他真的相信自己的展品是一个真实的世界，因为展品的拆除才出现了炸弹的新闻。抑或他真的回到了过去，炸弹的出现才是导致未来社会高压的原因。 评论 改编剧中几乎没有留下什么，但两篇的主题都是“变换的现实”——一个 PKD 作品中经常出现的主题。如果我们对于现实的认知不能超过感官所及，那么又怎能区分“真实的现实”和“虚拟的现实”。《黑客帝国》中探讨过相同的主题，然而这一篇与《攻壳机动队》中更进一步，如果连思想和记忆都可以进行虚拟，不仅世界的真实性变得不可靠，连自我的真实性都无法决定了。 看过《银翼杀手》的同学一定会对此感到熟悉，仿生人通过虚假记忆不知道自己是仿生人，这是一个核心设定，《银翼杀手2049》中，仿生人知道自己是仿生人，且知道自己有虚假的记忆，一下将揭示虚假记忆所展现的力量削弱到几乎没有了。 PKD 自己也经常怀疑所处的世界是否为真实，一如他笔下的人物，他又无法最终抉择究竟哪种世界为真，因此作品的结局常常是含糊的。改编中给出了明确的答案，因此转变成了对虚拟现实这一万能许愿机的警告。 此外，在面对多个可能的真实时，人总是需要选择一种“自己的真实”，也即相信哪一个真实才是“真实的真实”。在选择的过程中，一种标准是，一定要活在真实中，也就是选择最像真实的那个真实。另一种标准是，只要活在自己最喜欢的真实即可。除了选中的真实以外，其他世界中的人都会觉得你在有意无意地“欺骗自己”。 虽然两个故事都没有展示，不过发现自己的选择错误也是一个有趣的点。比如剧中，与虚拟人物热情相拥，但却看到整个世界渐渐陷入黑暗，会怎么想呢？后悔自己的决定？还是坚持自己的选择？ PKD 提出了问题，意识到自己没有能力去回答，因此他提出了更多的问题。 《电子梦》的执行出品人 Ronald D. Moore （《太空堡垒卡拉狄加》、《星际迷航7》、《星际迷航8》、《星际迷航：下一代》）为本篇写了导读。他在其中表达了对今年来虚拟现实技术的关注，因此将 VR 项目引入到了故事中。实际上，他自己作为编剧写过一部探讨虚拟现实的电影 Virtuality 。 PKD 的时代已经出现了 VR 技术的先声，尽管 PKD 并没有明确的展示使用何种设备进入虚拟的现实。但他的作品无疑谈及了更广泛意义上的模拟现实（Simulated Reality），甚至不是模拟，而是另一种完全不同的现实。","categories":[{"name":"科幻","slug":"科幻","permalink":"https://crvdgc.github.io/categories/科幻/"},{"name":"评论","slug":"科幻/评论","permalink":"https://crvdgc.github.io/categories/科幻/评论/"}],"tags":[{"name":"pkd","slug":"pkd","permalink":"https://crvdgc.github.io/tags/pkd/"},{"name":"剧集","slug":"剧集","permalink":"https://crvdgc.github.io/tags/剧集/"},{"name":"cyberpunk","slug":"cyberpunk","permalink":"https://crvdgc.github.io/tags/cyberpunk/"},{"name":"虚拟现实","slug":"虚拟现实","permalink":"https://crvdgc.github.io/tags/虚拟现实/"}]},{"title":"为字体添加powerline支持","slug":"add-powerline-support","date":"2018-05-04T00:10:52.000Z","updated":"2019-04-25T17:41:42.757Z","comments":true,"path":"2018/05/04/add-powerline-support/","link":"","permalink":"https://crvdgc.github.io/2018/05/04/add-powerline-support/","excerpt":"","text":"vim-airline是一款不错的vim实用工具。还可以通过定制主题进行美化。 看到一款不错的主题安装后，却出现了奇怪的情况。 这效果完全不一样好吗！ 查阅资料得知，这是字体的问题。 众所周知， Vim 是一个以命令行为主的编辑器，我用的 neovim 更为如此。那么如何在命令行中画出 UI 需要的图形呢？这里采取的方式是，使用绘图字符（ glyph ，又称字形）。比如「■」的字符。如果这个字符在当前环境下未定义，或者定义成别的了，就会出现上面的错误。 Vim 原来有一个基于 Python 的项目叫 powerline （现已停止开发），可算作 vim-airline 的精神前身吧。为了画出美观的 UI ，自己定义了一部分绘图字符，把它们放在了没有使用的 Unicode 编码中。 后来大家发现了绘图字符在命令行下绘制 UI 的简洁程度（最基本的只有 7 个），于是很多命令行应用也使用这些字符来绘制自己的 UI 了。而这些字符就被统称为 powerline glyphs 。 后来有人仿照这个，继续扩展命令行字符集，你可以在 nerdfonts 中找到 40 多种， 3000 多个字符。正如名字暗示的那样，里面包含了各种各样的 nerd stuff ，比如各种技术标志，甚至还有 Spock 手。如果 emoji 由 nerds 发明的话，大概就是这个效果吧。 由于这些字符不是标准编码，想要使用这些字符，需要字体在这些编码上正确定义。比如 powerline/fonts 给出了很多打好补丁的字体。 如果纯英文环境的话，常见的字体都包括了。但代码有中英混排需求，而中文字体又少有打好补丁的。这时可以选择上面 nerdfonts 提供的打补丁程序，给自己的字体打上补丁。 注意运行完成后得到的字体会被更名，以和原字体区分开。一般会以 Nerd Font 结尾。 如果不想更改各种软件的设置的话，可以通过编辑字体元信息使得名称和原字体一样。并用新字体将原字体覆盖。 最终得到的效果如下： 我平常使用 Consolas-with-Yahei 字体，打好补丁的版本存放在 crvdgc/Consolas-with-Yahei ，欢迎使用。","categories":[{"name":"计算机","slug":"计算机","permalink":"https://crvdgc.github.io/categories/计算机/"},{"name":"工具","slug":"计算机/工具","permalink":"https://crvdgc.github.io/categories/计算机/工具/"}],"tags":[{"name":"字体","slug":"字体","permalink":"https://crvdgc.github.io/tags/字体/"}]}]}