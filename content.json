{"meta":{"title":"Down to the Rabbit Hole","subtitle":"放映机 方向键 二进制 多巴胺","description":null,"author":"crvdgc","url":"https://crvdgc.github.io"},"pages":[{"title":"about","date":"2018-07-24T09:51:30.000Z","updated":"2018-07-28T17:39:28.000Z","comments":true,"path":"about/index.html","permalink":"https://crvdgc.github.io/about/index.html","excerpt":"","text":"Down to the Rabbit Hole 兔子洞中到底有什么呢？ 这是一个科幻＋计算机爱好者的小站。 关注二进制、放映机、方向键和多巴胺的一切。 联系方式 Github: https://www.github.com/crvdgc/ 微信: yokis1997 邮件（推荐）: yokis1997@pku.edu.cn 友情链接： 微信公众号： 老火箭酒吧 laorocketbar"}],"posts":[{"title":"Poorman's PageRank | 从零开始 Haskell 实现 PageRank","slug":"poorman-pagerank","date":"2019-04-26T03:00:20.000Z","updated":"2019-04-26T04:29:19.561Z","comments":true,"path":"2019/04/26/poorman-pagerank/","link":"","permalink":"https://crvdgc.github.io/2019/04/26/poorman-pagerank/","excerpt":"","text":"PageRank 算法是一种经典的网页排名算法。基本思想是，每个节点首先赋相等的初值。接下来，根据链接关系将值传播到链接去的节点。如此迭代直到收敛。 需要特殊处理的地方是，出度为 0 的节点需要将值保存到自己。 为了避免自私的节点不引用别人，从而大量积累自己的值，进行平滑处理。给每一个节点乘以缩减因子 $s$ ，再将每个节点加上相等的 $(1-s)/n$ 。注意到这种平滑不改变总值。也即任何时刻所有节点的值之和恒为 1 。 与之相关的还有 特征向量中心度 eigenvector centrality ，其区别是，不处理出度为 0 的点，也不进行平滑。而在每一步进行正规化。此外，特征向量也可以使用入度作为标准，仅需将连接矩阵转置即可。 这里给出一种简洁的三合一 Haskell 实现。不使用任何复杂的库函数，仅用 80 行。从中可以看到 Haskell 的简洁和抽象能力。 三种算法的核心都是不断迭代直到收敛。将这一逻辑抽象出来得到： 12345converge :: Eq a =&gt; (a -&gt; a) -&gt; a -&gt; aconverge f v = fst $ until theSame update (v, f v) where theSame (x, y) = x == y update (x, y) = (y, f y) 这里用到了库函数 until :: (a -&gt; Bool) -&gt; (a -&gt; a) -&gt; a -&gt; a 。这个函数接收一个判断函数，一个更新函数和初值。当判断函数返回假时，会应用更新函数。当判断函数返回真时，返回最终值。 converge 函数实际上要构造一个流（stream），即 v : f v : f (f v) : f (f (f v)) : ... 。当流的两个连续元素相等时，我们找到了 f 这个函数的不动点，也就是最终的收敛值。 因为只需要比较前两个元素，所以我们使用两个元素的元组（tuple）作为保存的状态。until 的判断函数就是两个元素是否相等。更新函数是抛弃第一个元素，对第二个元素应用 f 。 接下来不同算法的区别，仅在更新函数不同。 对于 pageRank 来说，就是不断乘以连接矩阵： 1234pageRank :: [[Value]] -&gt; [Value] -&gt; [Value]pageRank a vs = head $ converge (`matmul` a') [vs] where a' = compensate a 其中 matmul :: (Num a) =&gt; [[a]] -&gt; [[a]] -&gt; [[a]] 是矩阵乘法，将在下面给出实现。 注意到，首先将初值用列表改成 (n, 1) 的行向量，因此每次迭代改为右乘连接矩阵。最后使用 head 再转变成一维列表 (n,) 。下面各个算法做同样的处理。 compensate 函数实现两个功能，对于出度不为 0 的节点，将因子 1 平均分配到每个非零节点上；对于出度为 0 的节点，将 1 分配到自己的位置上（矩阵对角线）。 123456789101112131415161718compensate :: [[Value]] -&gt; [[Value]]compensate = map procOut . zip [0 ..] where procOut (i, l) = if any (/= 0) l then distribute l else oneAt i l distribute l = let v = 1.0 / (sum l) in map (\\x -&gt; if x == 0 then x else v) l oneAt i l = let (x, _:ys) = splitAt i l in x ++ 1.0 : ys 平滑处理可以改为对连接矩阵进行修改： 12345smooth :: Value -&gt; [[Value]] -&gt; [[Value]]smooth s m = map (map interpolate) m where interpolate a = s * a + (1.0 - s) / fromIntegral n n = length m 对每一个元素，都用因子 s 缩减，再加上补偿。 那么平滑后的 PageRank 算法如下： 1234smoothPageRank :: Value -&gt; [[Value]] -&gt; [Value] -&gt; [Value]smoothPageRank s a vs = head $ converge (`matmul` a') $ [vs] where a' = smooth s . compensate $ a 对于特征向量中心性，需要实现正规化： 1234normalize :: (Fractional a, Ord a) =&gt; [a] -&gt; [a]normalize vs = let m = maximum . (map abs) $ vs in map (/ m) vs 即将一个行向量的每个元素除以最大值。 那么特征向量中心性可以实现如下： 123eiginCentr :: [[Value]] -&gt; [Value] -&gt; [Value]eiginCentr a vs = head $ converge ((map normalize) . (`matmul` a)) [vs] 以上已经实现了三个算法的核心部分。接下来给出辅助函数的直观定义。 矩阵乘法： 12345678dot :: (Num a) =&gt; [a] -&gt; [a] -&gt; adot x y = sum $ zipWith (*) x ymatmul :: (Num a) =&gt; [[a]] -&gt; [[a]] -&gt; [[a]]matmul a b = map rowMul a where b' = transpose b rowMul r = map (dot r) b' 类型转换： 1234type Value = DoubleaFromIntegral :: (Integral a) =&gt; [[a]] -&gt; [[Value]]aFromIntegral = map (map fromIntegral) 生成初始平均分配值： 12normalDist :: Int -&gt; [Value]normalDist n = replicate n $ 1.0 / fromIntegral n 图从边表示转化为连接矩阵表示： 12345678910edgeToAdj :: (Integral a) =&gt; [(a, a)] -&gt; [[a]]edgeToAdj es = [[query i j | j &lt;- [0 .. upper]] | i &lt;- [0 .. upper]] where (ls, rs) = unzip es vs = ls ++ rs upper = maximum vs -- lower bound = 0 query i j = if elem (i, j) es then 1 else 0 其实这里使用 ST monad 更好一点，仅需要 $O(v^2)$ 的时间复杂度。这里用的是直接搜索，需要 $O(v^4)$ 的时间复杂度。 以上代码实现了所有三个算法的功能，仅用了 80 行代码。完整代码见 gist 。 使用下图进行测试： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758-- Test Graph 2tg2e = [ (0, 8) , (1, 6) , (1, 10) , (1, 11) , (2, 1) , (2, 10) , (2, 11) , (3, 15) , (3, 17) , (4, 1) , (4, 6) , (4, 15) , (5, 7) , (5, 8) , (5, 16) , (6, 5) , (6, 8) , (6, 16) , (7, 5) , (7, 13) , (7, 15) , (8, 16) , (8, 5) , (8, 6) , (9, 11) , (9, 10) , (9, 2) , (10, 9) , (10, 11) , (10, 13) , (11, 9) , (11, 10) , (11, 15) , (12, 13) , (12, 15) , (12, 16) , (13, 14) , (13, 15) , (13, 16) , (14, 13) , (14, 12) , (14, 15) , (15, 1) , (15, 9) , (15, 11) , (16, 7) , (16, 8) , (16, 13) ]tg2 = edgeToAdj tg2etg2spr = smoothPageRank 0.8 (aFromIntegral tg2) (normalDist . length $ tg2)printTg2spr :: IO ()printTg2spr = mapM_ (printf \"%.3f\\n\") tg2spr 测试结果如下： 123456789101112131415161718192021222324$ stack ghciλ&gt; :load pagerank.hs[1 of 1] Compiling Main ( pagerank.hs, interpreted )Ok, one module loaded.λ&gt; printTg2spr0.0110.0490.0340.0110.0110.0540.0450.0480.0690.0870.0840.1040.0200.0830.0330.0950.0830.078λ&gt; 符合预期。 连矩阵乘法都从头开始写，到整个算法完成，仅需要 80 行代码。核心就是 converge 函数的抽象。这个例子很好地体现了 Haskell 作为函数式语言的优点。","categories":[{"name":"计算机","slug":"计算机","permalink":"https://crvdgc.github.io/categories/计算机/"},{"name":"函数式程序设计","slug":"计算机/函数式程序设计","permalink":"https://crvdgc.github.io/categories/计算机/函数式程序设计/"}],"tags":[{"name":"haskell","slug":"haskell","permalink":"https://crvdgc.github.io/tags/haskell/"},{"name":"social-network","slug":"social-network","permalink":"https://crvdgc.github.io/tags/social-network/"},{"name":"math","slug":"math","permalink":"https://crvdgc.github.io/tags/math/"}]},{"title":"Poorman's SE VCG | 从零开始 Haskell 实现搜索引擎 VCG","slug":"poorman-vcg","date":"2019-04-25T17:44:53.000Z","updated":"2019-04-26T04:30:08.825Z","comments":true,"path":"2019/04/26/poorman-vcg/","link":"","permalink":"https://crvdgc.github.io/2019/04/26/poorman-vcg/","excerpt":"","text":"VCG 即 Vickrey–Clarke–Groves 机制，是一种具有很多优秀性质的拍卖机制。包括完美匹配、市场出清、在所有可能的清仓价格中总价格最低、总收益达到社会最优、买家按真实估值出价是出价均衡而且是最优策略等等。 搜索引擎广告位拍卖是一种特殊的拍卖形式。详细分析见《网络、群体与市场》 第 15 章。（英文预印本可在 官方网站 合法下载。）假设广告位 $i$ 的点击率 $r_i$ ，广告商 $j$ 对单点击的估值为 $b_j$ 。那么广告商 $j$ 对广告位 $i$ 的总估值 $v_{ij} = r_i b_j$ 。 注意到同一个广告位的点击率对所有广告商相同，因此对单点击估值最高者获得点击率最高广告位，依此类推。这就为搜索引擎中的 VCG 计算带来的简化的空间。 每个广告商所支付的价格等于假设它不出现时，其他广告商所能获得的总福利增加。（也就相当于它出现给其他所有人带来的总损失。） 不妨将广告从点击率高到低排列，广告商按估值从高到低排列。那么估值最高者所支付价格等于第二高的广告商如果取得第一高的广告位能增加的福利（$(r_1 - r_2) b_2$），加上第三高的广告商取得第二高的广告位能增加的福利（$(r_2 - r_3) b_3$）等等。也就是估值矩阵（$v_{ij}$）主对角线减次对角线。 这里给出一种超轻量的 Haskell 实现。 首先对原公式进行变形： $$\\begin{align} p_j &amp;= \\sum^{m}_{i=j+1} b_i(r_{i-1} - r_i) + b_{m+1}r_m \\\\ &amp;= \\sum^{m+1}_{i=j+1} b_i(r_{i-1} - r_i) \\end{align}$$ 其中 $r_{m+1} := 0, b_{m+1}:= \\mathbf{if} \\ m=n\\ \\mathbf{then} \\ 0 \\ \\mathbf{else} \\ b_{m+1}$ 可以将原有的点击率列表和每点击估价列表，通过填充 $0$ ，将边界情况统一成一般情况。 给定某广告位之后的点击率列表和估价列表，计算该广告位价格的方法如下： 12345price :: [Int] -&gt; [Int] -&gt; Intprice rs bs = sum $ zipWith (*) padbs padrs where padbs = tail $ bs ++ [0] padrs = zipWith (-) rs $ tail rs ++ [0] 首先，padbs 根据上面的推导分别在原有列表尾部添 0 。如果广告商不足（$m=n$），则会使用多余的 0 ；如果已经足够（$m&gt;n$），则添加的 0 会在 zipWith 中被舍弃。 填充之后，padbs 进行了 tail ，这是因为每个广告上的价格是靠其后广告商估值计算得到的。 padrs 负责计算当前广告位点击率和下一个广告位点击率之差。 而主函数部分只是一个简单的向量内积。 price 函数整体上实现了，给定某一广告位开始（包括自己）的点击率列表，和估值列表，可以计算这一广告位的价格。 计算所有广告位的价格的方法如下： 12vcg :: [Int] -&gt; [Int] -&gt; [Int]vcg rates bids = init $ zipWith price (tails rates) (tails bids) 这里利用了 tails :: [a] -&gt; [[a]] 函数，需要 import Data.List (tails) ，其作用是接收一个列表，不断去除首元素，再将结果收集成一个列表。 例如： 12λ&gt; tails \"abc\"[\"abc\", \"bc\", \"c\", \"\"] 函数首先生成了每个广告位开始（包括自己）的点击率和估值列表，再将其作为参数传给 price 。最后加 init 是因为 tails 函数会将空列表作为最后一个元素。我们将这个价格抛弃。 测试如下： 1234λ&gt; vcg [5,3,1] [15,8,5][26,10,0]λ&gt; vcg [5,1] [11,7,5,3,2,1][33,5] 实现正确。 有效代码只有 4 行，只用了一个简单的库函数。这个例子很好地体现了 Haskell 的简洁和抽象能力。 注：题图来源 https://commons.wikimedia.org/wiki/File:Tea_auction_australia.jpg ，授权 CC BY 2.0 ，作者 State Library of Victoria Collections","categories":[{"name":"计算机","slug":"计算机","permalink":"https://crvdgc.github.io/categories/计算机/"},{"name":"函数式程序设计","slug":"计算机/函数式程序设计","permalink":"https://crvdgc.github.io/categories/计算机/函数式程序设计/"}],"tags":[{"name":"haskell","slug":"haskell","permalink":"https://crvdgc.github.io/tags/haskell/"},{"name":"social-network","slug":"social-network","permalink":"https://crvdgc.github.io/tags/social-network/"},{"name":"math","slug":"math","permalink":"https://crvdgc.github.io/tags/math/"}]},{"title":"【有剧透】奇异人生 - 人类的能力是有极限的","slug":"奇异人生评论","date":"2018-10-27T17:30:19.000Z","updated":"2019-04-26T01:47:05.689Z","comments":true,"path":"2018/10/28/奇异人生评论/","link":"","permalink":"https://crvdgc.github.io/2018/10/28/奇异人生评论/","excerpt":"","text":"版权归作者所有，任何形式转载请联系作者。 作者：D.B.（来自豆瓣） 来源：https://www.douban.com/review/9729171/ 12345\\\\\\ /// 剧透预警/// \\\\\\ 包含「奇异人生」的主要剧透 「奇异人生」对细节的雕琢和对场景的表现十分精彩，结局却没能让所有人满意。无论之前做出了何种选择，最后却归结到简单的二选一上。至少故事是这么展现的，也希望让你相信是这样的。但这个选择真的有意义吗？让我们倾听一下理性之声吧。 选择有意义的前题是，对选择可以造成的后果有比较完全的知识。 让我们先看一个买彩票的例子。 一个数学家去买彩票，买了「00 00 00……」这样的号码。旁边的彩民感觉很奇怪，因为他/她每次购买的都是精心挑选的一系列数字。她/他认为，数学家的选择会造成中奖率降低，因此他/她认为数学家的选择是不对的。 更加理性的数学家会发现，假设彩票的数字真的是随机抽出来的话，那么不同的选择造成的后果其实没有差别。选择「00 00 00……」只是个人偏好，外加节省力气而已。 当然，这么做合理的前题是，彩票真是随机抽出来的。假设彩票公司动了手脚，问题就转化为你对彩票公司抽奖机器的掌握程度了。显然，信息越多，越有助于做出有利的决策。 彩票和奇异人生有什么关系呢？ 还记得结局的两个选择么？要么牺牲 Chloe 保护小镇，要么救下 Chloe 牺牲小镇。但真的是这样么？ 做出选择的依据在哪里？要知道，「蝴蝶效应」类故事的一个关键在于混沌理论，也就是初始条件微小的差异会造成巨大的，难以预计的后果。这种对确定性的破坏，保证了人们难以做出完美的选择，从而形成了故事的主要矛盾。 因此，你的两个选项并非「要么 Chloe ，要么小镇，其他完全相同」（如电车难题一样），而是「 Chloe 外加这一事件的未来，和，小镇外加这一事件未来」。要知道影响可能持续成千上万年，甚至改变整个人类的历史进程。怎么样？有没有感觉肩上的担子稍微沉了一些？ 在如此巨大且不可知的未来中，几条人命又算得了什么呢？但如果不考虑这些的话，我们又能考虑什么呢？无论如何选择都会有风险，微小的差异可能改变一切。如此不可知、多变、又危险的后果，很难想象怎么做出「正确的选择」。 因为要在混沌中正确，就要求近乎「全知」的能力。而人类的能力是有极限的，你不能拯救所有人（某切丝做出了错误示范）。在这种情况下，无论如何选择，很难分辨到底哪种是「道德」的，或者说「正确」的。其实只是一个个人偏好问题。你喜欢人多热闹一点，就牺牲 Chloe 。你喜欢 Chloe 就留下她。 如果有人问，为何你如此自私，牺牲那么多人，就为了一个人？那你可以回答，有可能我救下一个镇子的人，未来人类会死于热核战争。我说不准，你也说不准。所以，其实，我爱怎么样就怎么样吧。 等等，这句话怎么这么像 Rick &amp; Morty 里的台词？没错，由混沌理论带来的不可知论，消灭了选择的意义，也就消灭了道德的意义。本身带有强烈的虚无主义色彩。在这种世界中，意义不会来自于外部。所以，你只能选择要么像 BoJack 一样，沉溺于虚无，要么像 Rick &amp; Morty 或者尼采那样，嘲笑虚无，自己为自己设立意义并乐在其中。 这样的说法也可以说是顺其自然，因为无论你有没有回溯能力，做出的每一个微小的改变都有同样的影响，试图修正是很困难的。 因此，当有人（比如制作组）惊讶于你竟然选择了 Chloe 时，你可以说，「我只不过在彩票上买了一串 0 而已」。 最后补充一点，我知道故事的逻辑和现实的逻辑不一样。故事的世界不遵循理性，因为它的力量正源于此，说让你二选一，其他都一样，就可以做到其他都一样。说白了，故事世界的上帝，也就是制作人，是个混蛋一样的上帝，只有如此，才能让里面的角色展现自身。所以这里只是如果从现实的角度看待会怎样的想法。","categories":[{"name":"科幻","slug":"科幻","permalink":"https://crvdgc.github.io/categories/科幻/"},{"name":"评论","slug":"科幻/评论","permalink":"https://crvdgc.github.io/categories/科幻/评论/"}],"tags":[{"name":"游戏","slug":"游戏","permalink":"https://crvdgc.github.io/tags/游戏/"}]},{"title":"Adorable lores","slug":"adorable-lores","date":"2018-08-11T13:09:09.000Z","updated":"2018-08-11T13:35:50.000Z","comments":true,"path":"2018/08/11/adorable-lores/","link":"","permalink":"https://crvdgc.github.io/2018/08/11/adorable-lores/","excerpt":"","text":"前言 lore 这个词在日常中不大常见[1]。见得更多的，可能是它的亲戚 folklore （民俗）。不过在科幻奇幻世界（universe）构建，以及游戏背景中经常可以看到这个词。它到底是什么意思呢？和一般的 story 有什么区别？二者哪个更好？我们能从已有的例子中学到什么构建世界以及叙述上的技巧呢？ 本篇博文将通过几个例子，尝试回答以上问题。可能是因为在学 GRE 吧，结构写得像中学作文，不过我发现这是最有效的交流方式。 注意，前方将有 黑暗之魂 1 和 3 、 银河帝国系列 的主要剧透（达到影响首次体验程度）。此外，还有 冰与火之歌系列、VA-11 Hall-A: Cyberpunk Bartender Action、 差分机 、 高城堡里的人 的轻微剧透（不多于书的腰封、游戏简介的程度）。 我已经警告过你了 The Core of the Lore 世界不是从一片混沌中忽然出现的。无论事物现在呈现什么状态，它们一定不是亘古不变的，发展成为现在的样子一定有其原因。 ——奥森·斯科特·卡德[2] 从词源上看，lore 本意包含 teaching 。词义是关于某一主题的传统或知识，通过教育或经验，随着时间的累积获得。在幻想世界中，与 backstory 接近，时间上位于现在发生的故事之前。形式上可以是故事，比如某个人物的起源故事（origin story）；或者是某一类人对其他人和物的态度，比如神话等。 从这个角度来看，lore 为当前故事提供了一个语境。单纯的 exposition 只描述了世界现在的状态，而 lore 解释了来源。可以说是在构建世界的硬件背景（地理环境、科幻/奇幻设定）之上的软件背景。不过 lore 本身并不是设定。相比之下，更关注生活在世界中的人们。Lore 体现了人们对世界的认知。Lore 可以说是一种集体无意识产物，神话（myth）是 folklore 中比较重要的一种，不过一般的 lore 不一定要包含“神”，也可以是传奇（legend）。 我们通过 lore 来一窥世界中人们的精神状态。我们关心世界，不过更关心 lore 中的人，以及讲述、学习 lore 的人。我们知道他们对世界的认识，从而了解他们的性格，他们如何行动，他们做出选择的原因，以及每个选项上承受的重量。 不了解 lore ，我们同样可以读故事（story）。毕竟，故事只是时间上连续发生的事件而已。不过 lore 为我们提供了深入理解其情节（plot）的机会。神话是社会向其组成个体解释自身的故事。通过了解神话与 lore ，我们得以聆听这种教诲，甚至成为这一虚拟社会中的一员。 由此， lore 可以大大增强故事的沉浸感。不只是 知道 ，而是 懂得 ，甚至于 进入 。这样的体验，对于科幻和奇幻迷来说，难道还不够激动人心么？ Lore 在小说的应用由来已久。近来，它们在游戏中的作用也逐渐显现。下面将举例说明这两点。这篇博文 将 lore 与传统的故事叙事（里面称作 cinematic story-telling）进行对比，其中的观点在下面也会谈到，推荐阅读。 Explore the Lore 小说 这里统一指小说、电影、漫画等线性文本。其中对 lore 的运用实在是太多也太古老了。即使是现实主义小说，也包含着 lore 成分在其中——只不过不是构建出来的，而是真实世界已经存在的。这样的 lore 自不必说，已经埋藏在文本之中了。而虚构世界也需要虚构的 lore ，运用得当，则作用巨大。 Lore 让故事更加有趣。 银河帝国 系列中，阿西莫夫将神话学与民俗学玩得出神入化，整个小说当做历史小说来读也完全没有问题。不仅每个故事中，众多 lore 交织。更神奇的是，由于小说的时间跨度，前面的人物成为了后面的 lore 。而读者作为“历史的亲历者”，看到故事中的后人对前人世界的敬仰或者误解，不仅享有知道真相的快感，也理解了后人对历史的态度。“你们的生活是一个谎言”，有时会想大声地对人物喊道。不过，谁又能知道过去全部的真相呢？我们也是听着各种各样的 lore 长大的。从这种反思中，我们得以从传统的视角中跳脱出来，不是以怀疑论者的身份，而是一个“趣味猎人”的身份，问出那个问题：“凯撒真的存在过么？” 好的 lore 会增强一致性（consistency），让虚构的故事真实可信。故事本身是关于特定时间、特定地点的一系列事件的。但当一个故事成为了 lore 后，其影响会无处不在。你可以从不同地方看到了故事产生的各种影响——它们或者指向不同的角度，或者揭示了真相。你还可以从中观察到每个人的反应和态度，从而更加清晰地勾勒出了其性格。奇幻小说天然与 lore 相性相合，比如 猎魔人 、 指环王 等。 冰与火之歌 构建世界的方式正是如此。当一个重要事件发生时，各种人，各种事件中，都会体现其影响，就如同真实的世界一样。影响的范围从王公贵族、神职者、骑士、马夫、乃至乞丐、盗贼；从事件的发生地，到遥远的地方；从现在，到多年以后。再加上冰火独特的轮换视角叙事方式。我简直想不明白为什么还没有历史学家的博士论文写维斯特洛。 或然历史（alternative history）类的幻想小说中，两个世界的 lore 相通。我们的历史自动成为他们的历史，而我们的世界却与他们的世界如此不同。正因为我们了解 Ada Lovelace 的故事， 差分机 中的角色才更加有趣。我们知道第三帝国的恐怖政策，因此在 高城堡里的人 中，它们成为现实才显得格外恐怖。 游戏 游戏的核心是交互性（interactivity）。我想你已经发现它与故事（story）的矛盾了。 首先，游戏不是必须有故事。想想 吃豆人 ，人们不想知道，也不需要知道吃豆人为什么要吃豆，幽灵为什么要追吃豆人。人们想要的只是摇杆。 其次，故事反而会伤害交互性。为了达到最佳叙事效果，故事的时间、地点、背景，人物的性格、行为方式都需要精心选择。限制选择，也就限制了交互的自由，也就损害了游戏性。 使命召唤 系列最为评论家厌弃的就是其电影脚本式的 单向线性 结构。没有什么挑战的内容，也就无从发挥创造力，玩家所做的不过是在两段 CG 动画中间，跟着各种箭头和标志，完成最简单的动作，毫无乐趣可言。 与之相对的，最好的游戏是完全的开放世界（open world）游戏么？大家玩过的最自由的开放世界游戏，是现实世界。不过大多数人不会觉得它比某些游戏更具吸引力（否则游戏产业就不复存在了）。可见，如果没有一定的精心的调整与选择，游戏也不会那么好玩。 我认为，这就是 lore 最具优势的地方：达到了故事与交互性的平衡。 Lore 本身是故事，是“不必要的故事”，同时也是“有了更好的故事”。以 黑暗之魂 中的 lore 为例，这个游戏将其发挥到了极致。 “什么？ 黑暗之魂 有剧情吗？”说存在当然是存在的，不过你可以完全不看。出色的交互设计本身就让它成为一款好游戏。相应的，主线故事简单：主角需要完成几项小任务，从而完成一个大任务。 反而是 lore 构成了黑魂真正的剧情。每一件物品都有对应的描述，显然不是给剧中角色看，而是给玩家看的。然而每一件的描述又不那么完整，像是零碎的点。但当你收集到足够多后，就可以像福尔摩斯一样将其联系起来了。合在一起，它们讲述了各种各样的 lore 。 它们是可选的，是的，不过它们也是“有了更好的”。你在打各式各样的龙类生物时，会在武器上抹上黄金松脂，从而造成更大的伤害。不是因为你试了各种松脂，或者查了攻略，而是因为你了解古代的战士，使用雷电的力量进行猎龙的传说。使用雷电，代表了你已经成为了这个世界的一部分。你表达了对世界传统尊敬与传承，而这个世界也因此奖励你——让作战变得更容易了，同时也展现了其内在一致性。 你了解角色的背景，他们的为人行事，会对游戏产生新的认知。当你听到了鲁道斯的痛苦、防火女的折磨、巨人王尤姆的悲剧时，你会对自己的使命产生怀疑，这也许会让你做出不同的选择，也许不会。但不同的是，你已经了解了这个世界，进入了这个世界了。 在这一刻，玩家与角色的割裂不再存在，你不是坐在屏幕前“打怪”、“通关”，而是真正融入到了世界之中，你和你所扮演的角色一样了解世界的过去，世界中的人。而这一切，不是靠 CG 或脚本强加于你的，而是你自己的选择。如此， lore 成为了游戏性与故事性的平衡点，也可以达到理想的角色扮演游戏效果。故事的背景、人物通过 lore 实现，而你在此基础上，理解了你的人物，从而做出选择，完成自己的故事。而不是因为你根本不知为何存在的选项限制，也不是毫无限制的任意开放（如同厨师让你自己决定所有调料用量一样不靠谱）。 此外，探索 lore 的过程本身就是一个小解谜游戏，自有其中的乐趣。黑魂使用 lore 完成的是背景介绍。而 VA-11 Hall-A: Cyberpunk Bartender Action 中，当前时间的叙事也都是靠类似于 lore 的方式得来的。你作为一个酒保，从客人口中，从新闻软件、在线论坛中，看到同一件事的不同侧面，从而像拼图一样，还原出事件的真相，勾勒出了一个完整的、令人信服的赛博朋克世界。 最后，系列小说/游戏中，前作的故事成为了后作的 lore ，可算得上是一个自我指涉的粉丝福利。 More on the Lore 看完了 lore 的作用，那么如何在创作中使用 lore 呢？ 首先，要保证一致性。至少作者心中应当知道事件的真相。如果不同的叙述需要出现矛盾，应当有理有据。如果自己先历史虚无了，那读起来可信度也会下降。 其次，和其他所有设定一样，不要“老教授的演讲”，也就是一大段对设定的解说，两个本身知道这件事的人，非要说一遍，只是为了给读者进行 exposition 。找一个“读者的眼睛”，一个“华生”，新来的人，失忆的人，记者，解释给他/她听。更好的方式是，在叙述中自然带出，“show, do not tell”。 此外，读者应该能通过线索大体还原出来，或者至少形成自己的理论。缺失一大块重要部分不一定能增加神秘感，反而可能让人失望。当然也别太容易。 lore 中的人或事物在故事中出现通常是个好主意。产生一种神话照进现实的超现实效果。 结语 一篇好好的杂谈写得像论文/作文一样。最后闲谈几句。 我特别喜欢有 lore 的作品，如果知道请务必推荐给我。此外，正如中文科幻圈中有用 PKD 作为菲利普·K·迪克的简称一样，我一直致力于推广自己发明的“奥森·斯科特·卡德”的简称——奥斯卡。为了让这条迷因扩散，我一定要在这里提一句。 https://www.merriam-webster.com/dictionary/lore ↩ 奥森·斯科特·卡德. 如何创作科幻小说与奇幻小说[M]. 百花文艺出版社, 2015. ↩","categories":[{"name":"科幻","slug":"科幻","permalink":"https://crvdgc.github.io/categories/科幻/"},{"name":"评论","slug":"科幻/评论","permalink":"https://crvdgc.github.io/categories/科幻/评论/"}],"tags":[{"name":"游戏","slug":"游戏","permalink":"https://crvdgc.github.io/tags/游戏/"},{"name":"创作","slug":"创作","permalink":"https://crvdgc.github.io/tags/创作/"},{"name":"设定","slug":"设定","permalink":"https://crvdgc.github.io/tags/设定/"}]},{"title":"论文笔记： Improving Neural Language Models with a Continuous Cache","slug":"continuous-cache","date":"2018-08-11T13:03:55.000Z","updated":"2018-08-11T13:59:29.000Z","comments":true,"path":"2018/08/11/continuous-cache/","link":"","permalink":"https://crvdgc.github.io/2018/08/11/continuous-cache/","excerpt":"","text":"摘要 块引用代表评论 本文提出了一种类似于 cache 的简化 memory RNN 方法。访问 cache 的方式是隐藏层的点积。和直接使用 memory 的方法相比， cache 无需训练，代价较低，因此空间更大。 前言 序列模型在建立语言模型上的一个问题是无法根据近期历史调整（adapt to recent history）。一个解决思路是 memory ，通常需要学习一个参数化的读写机制（learn a parametrizable mechanism to read or write to memory cells）。高昂的代价限制了其 memory 大小和可使用的数据量。 本文提出的 Neural Cache Model 主要思想和 count-based model （n-gram） 中的 cache 方法类似，可以看作其连续版本。前者的解读请看 Kuhn 的 cache 模型 。 模型 思路 语言学知识表明，一个单词在文档中出现一次过后，更有可能再次出现。因此，除了全局模型以外，cache 储存部分近期历史。使用 cache 得到的结果与全局模型的结果进行差值（interpolate）得到最终结果。 优点有： 语言模型能高效适应新领域 见过 OOV (out-of-vocabulary) 单词一次后，即可预测 通过产生更一致（coherent）的数据，改善文档级别的长依赖识别 数学表示 $$p_{cache}(w|h_{1…t}, x_{1…t}) \\propto \\sum_{i=1}^{t-1} 1_{ {w=x_{i+1}}}\\ \\exp (\\theta h_t^\\mathsf{T} h_i)$$ 也就是，从 cache 中生成单词 $w$ 的概率正比于当前隐藏层状态 $h_t$ 与之前所有生成 $w$ 的隐藏层状态 $h_i$ 的点积相似度的指数之和[1]。 使用类似 attention 的方法，把单项操作转化为多项求和，是离散操作连续化的常用手段。见下方。 可见，隐藏层状态作为 cache 的索引，当前隐藏层与 cache 中隐藏层状态相似，会提高对应单词的产生概率。符合上面的思路，而且更易生成重复模式。 语言模型即为二者插值： $$p(w|h_{1…t}, x_{1…t}) = (1-\\lambda)p_{vocab}(w|h_t) + \\lambda p_{cache}(w|h_{1…t}, x_{1…t})$$ 另一种组合方式是 global normalization ： $$p(w|h_{1…t}, x_{1…t}) \\propto \\left( \\exp(h_t^\\mathsf{T}o_w) + \\sum_{i=1}^{t-1} 1_{ {w=x_{i+1}} }\\ \\exp (\\theta h_t^\\mathsf{T}h_i + \\alpha) \\right)$$ 其中参数 $\\alpha$ 起着和 $\\theta$ 一样控制权重的作用。 实验部分表明，前者效果更好，且更容易实现，因此下面的方法均指插值。 实验 PTB 在 PTB 上的实验表明已经达到 state-of-the-art 。 中等规模数据集 在中等规模数据集上进行实验。数据集使用 wikitext103 和 wikitext2 （训练集规模分别为 103M 和 2M ，测试集相同），还有 text8 （同样从 wikipedia 使用不同预处理得到）。 首先看可扩展性： Fig. 4 展示了和 unigram 加 cache 相比，本文模型效果和扩展性更佳。 Table 2 表明，和 LSTM baseline 相比，本模型在 2M 上提升 30% ，在 103M 上提升 16% 。符合 (Goodman, 2001)[2] 观察到的，在大数据集上，复杂技巧带来的提升会下降。和 Pointer Sentinel LSTM [3] 相比， cache 大小同样为 100 时，在 2M 上效果差不多。但 cache 方法计算代价低，因此很容易更大的 cache 。使用 2000 的 cache 大小相比，效果提升了很多。 另外，注意到用 103M 的 LSTM baseline 已经比 2M 上的复杂方法好很多了，因此认为需要在更大数据集上实验（虽然本文没继续做了）。 一个拿 103M 训出来的 LSTM 已经强过 2M 上比较复杂的模型了。说明数据真实王道。学术上用相同数据量比较，为了公平。实际做出产品的话，没人管公平不公平了。 结论 本文中的模型从技术上虽然属于 memory-augmented RNN ，但其结构可以避免学习内存查找部分（memory lookup component），因此计算代价十分低，也很容易加到现有的模型上。 Sainbayar Sukhbaatar, Szlam Arthur, Jason Weston, and Rob Fergus. End-to-end memory networks. In NIPS, 2015. ↩ Joshua T Goodman. A bit of progress in language modeling. Computer Speech &amp; Language, 2001. ↩ Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. ↩","categories":[{"name":"计算机","slug":"计算机","permalink":"https://crvdgc.github.io/categories/计算机/"},{"name":"人工智能","slug":"计算机/人工智能","permalink":"https://crvdgc.github.io/categories/计算机/人工智能/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"https://crvdgc.github.io/tags/RNN/"},{"name":"cache","slug":"cache","permalink":"https://crvdgc.github.io/tags/cache/"},{"name":"NLP","slug":"NLP","permalink":"https://crvdgc.github.io/tags/NLP/"}]},{"title":"Cache Method in n-gram","slug":"kuhn-cache","date":"2018-08-03T17:55:36.000Z","updated":"2018-08-03T18:02:05.000Z","comments":true,"path":"2018/08/04/kuhn-cache/","link":"","permalink":"https://crvdgc.github.io/2018/08/04/kuhn-cache/","excerpt":"","text":"前言 本篇简略介绍一下 (Kuhn, 1988)[1] 和 (Kuhn, De Mori, 1990)[2] 中的主要思想，即在 n-gram 基础的马尔科夫模型中引入 cache 机制的方法。作为讨论后续使用神经网络方法引入连续 cache 的序列模型的铺垫。 思想 为序列模型引入 cache 是基于这样的假设：近期使用的单词，出现频率比全局更高。这是一个受语言学启发的假设。 原语言模型基于 n-gram ，论文中提出的模型为其增加了 cache 部分。以 Part-of-Speech (POS) 作为线索，改进语言模型。这是基于另一个假设：一个内容词（a content word），比如特定的名词或动词，倾向于集中出现；而功能词（function words），倾向于平均分布。 模型的核心可以用以下公式概括： $$ P(W_i = W | g_i = g_j) \\approx k_{M, j} \\times f(W_i = W | g_i = g_j) + k_{C, j} \\times C_j(W, i) $$ 各项含义如下： $W_i = W$ 即第 $i$ 个位置上的单词是 $W$ $g_i = g_j$ 即第 $i$ 个位置上的 POS 是 $g_j$ $k_{M, j} + k_{C, j} = 1$ ，两项分别代表全局 Markov 模型部分概率权重和 cache 部分概率权重。注意到不同的 POS 对应不同的权重分配。 $f(W_i = W | g_i = g_j)$ 为全局 Markov 模型，这里使用的是 3g-gram 。 $C_j(W, i)$ 为从第 $j$ 个 POS 对应的 cache 中所得到第 $i$ 个位置单词为 $W$ 的概率，代表了语境信息。 进行序列预测时，每个 POS 维护一个 LRU 的 cache 储存一定量的单词。对于第 $i$ 个位置，预测其为单词 $W$ 的概率由前两个位置的 POS 产生各种 POS 的概率，乘以由上面公式计算出不同 POS 生成该单词的概率得到。 以上部分，略去了 trigram 预测 POS 的模型以及对 out-of-vocabulary 单词的处理。具体可见论文。 结果 实验证明了 cache 的有效性，并为内容词与功能词的语言学假设提供了实验支持。实验发现[2]，功能词全局部分比重较大，而内容词 cache 部分与全局部分比重相当。见上方表格 3 。 评论 对语境信息建模的方式很多，直接用 cache 储存起来算是很直接的想法。这篇论文[3]将 Kuhn 两篇中的思想改成了连续版本，依然保留了无需训练的优点。也许有的时候模型偏置（model bias）不需要对处理的过程太过干预，能为其提供获得想要信息的方法就好。 Kuhn, R. (1988). Speech recognition and the frequency of recently used words: A modified markov model for natural language. Proceedings of the 12th conference on Computational linguistics-Volume 1, Association for Computational Linguistics. ↩ Kuhn, R. and R. De Mori (1990). “A cache-based natural language model for speech recognition.” IEEE transactions on pattern analysis and machine intelligence 12(6): 570-583. ↩ ↩ Grave, E., et al. (2016). “Improving neural language models with a continuous cache.” arXiv preprint arXiv:1612.04426. ↩","categories":[{"name":"计算机","slug":"计算机","permalink":"https://crvdgc.github.io/categories/计算机/"},{"name":"人工智能","slug":"计算机/人工智能","permalink":"https://crvdgc.github.io/categories/计算机/人工智能/"}],"tags":[{"name":"cache","slug":"cache","permalink":"https://crvdgc.github.io/tags/cache/"},{"name":"language-model","slug":"language-model","permalink":"https://crvdgc.github.io/tags/language-model/"}]},{"title":"论文解读 Recurrent neural network based language model","slug":"rnnlm","date":"2018-07-31T10:38:18.000Z","updated":"2018-08-01T06:51:36.000Z","comments":true,"path":"2018/07/31/rnnlm/","link":"","permalink":"https://crvdgc.github.io/2018/07/31/rnnlm/","excerpt":"","text":"Annotation: Recurrent neural network based language model 作者 Tomas Mikolov Martin Karafiat Lukas Burget Jan “Honza” Cernock Sanjeev Khudanpur 摘要 块引用表示评论。 本文提出了一个基于 RNN 的语言模型（RNN LM）。实验表明与 backoff 语言模型相比，困惑度（perplexity）可能下降 50% 。 简单直接提出 RNN LM ，使用大量实验证明和 n-gram 相比效果不错（缺点是训练复杂度比较高）。 由于模型比较简单，因此在最后的评论中直接概括一下。这篇论文的引言写得十分精彩，对问题的分析一针见血。（当然说得这么坚定也有实验效果撑着呢，想必下笔的时候也是激动万分。）我十分喜欢，主要呈现一下这部分。 引言 构建语言模型，就是处理序列预测问题（sequential data prediction）。然而，很多自然语言方法都针对于特定的语言领域（very specific for language domain）：假设自然语言可以使用分析树（parse tree）来表示，需要考虑词的形态学（morphology）、语法和语义。即使是基于 n-gram 的最通用的模型，也进行了假设：语言是由原子性的符号（也就是单词）序列（也就是句子）所组成的。句子的结尾起着十分重要且特殊的作用。 特定于语言领域这个观察十分有道理。 n-gram 以句子为单位本身已经带有很强的假设，给予了“句子”一个很高的地位，因此也就无法对句间关系建模。然而衡量语言模型好像没有不用句子假设的，即使是下面提出的 RNN 也是如此。这一段可能是为了反衬 RNN 的泛用性。 对简单的 n-gram 研究到底有没有取得显著进步，值得怀疑。如果从序列预测数据的角度来看，的确取得了很大进步。主要靠 cache models （描述长语境信息）和 class-based models （通过相似词之间共享参数改进短语境的参数估计）。其他进步大多能归结到这两类的效果上。 如果从实际应用的角度来看，那么几乎没有进展。真实世界中的语音识别和机器翻译的系统都是建立在大量的数据上的，一种流行的说法是我们只需要更多的数据就够了。学术界的模型通常很复杂并且仅仅在基于数量十分有限的数据集上效果才好。事实上，大多数的先进技术只比简单的 baseline 提高了一点，且很少在实际中使用。 满满的即视感。不过 RNN 带来的提升的确离现实应用近了一大步。 评论 模型 本篇的模型十分朴素，是一个简单的三层 RNN 。Token 使用的是 one-hot 编码。输入层使用单词编码和隐藏层进行拼接。隐藏层使用 sigmoid 激活函数，输出层使用 softmax 。训练算法是 truncated backpropagation through time ， SGD 。如果没有明显改善，学习率每个 epoch 减半。 Dynamic 模型中一个比较有趣的地方（也是读这篇论文的原因）是使用了 dynamic 的方法。主要区别于传统的 static 方法。Static 指的是模型在训练阶段结束之后，将参数固定，在测试过程中不再改变。Dynamic 方法则是在测试时，利用训练的真实标签继续更新参数。 这种做法的一个结果是不再显式地区分训练集与测试集，因为所有的数据都只处理一次。 (Graves, 2013)[1] 中指出了 dynamic evaluation 比本篇论文报告的效果更好。 作者指出，效果和 cache 类似，但由于其在连续空间中学习，如果两个词之间联系比较紧密，那么测试数据中一个单词的频繁出现也会提高另一个单词出现概率。 另一篇专注研究 dynamic evaluation 的论文解读请看 。 全文 作者认为 RNN 相比于 Bengio [3][2] 中的 FNN 的主要优势在于没有指定固定的语境，而是使用隐藏层的状态概括之前所有的语境信息。优点包括需要指定的超参数数量少，通用性强。缺点是难以捕捉长依赖问题，早在 1994 年的 [6][3] 中就已经指出了。解读请看这篇博客。 本篇将 RNN LM 引入 NLP ，使用的是最朴素的模型（本文发表于 2010 年）。实验发现其效果远好于（各种） n-gram 。（从之后的发展来看，几乎将 n-gram 送入历史的废纸堆了）。这一巨大的提升，打破了语言模型是关于各种 n-gram 以及只要有大量的数据就可以提升效果的神话。（结果现在出现了各种复杂的神经网络模型，以及只要有大量数据就可以提升效果的神话x） Graves, Alex. “Generating sequences with recurrent neural networks.” arXiv preprint arXiv:1308.0850 (2013). ↩ Yoshua Bengio, Rejean Ducharme and Pascal Vincent. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137-1155 ↩ Yoshua Bengio and Patrice Simard and Paolo Frasconi. Learning Long-Term Dependencies with Gradient Descent is Difficult. IEEE Transactions on Neural Networks, 5, 157-166. ↩","categories":[{"name":"计算机","slug":"计算机","permalink":"https://crvdgc.github.io/categories/计算机/"},{"name":"人工智能","slug":"计算机/人工智能","permalink":"https://crvdgc.github.io/categories/计算机/人工智能/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"https://crvdgc.github.io/tags/RNN/"},{"name":"NLP","slug":"NLP","permalink":"https://crvdgc.github.io/tags/NLP/"}]},{"title":"论文解读 Learning Long-Term Dependencies with Gradient Descent is Difficult","slug":"learning-long-term","date":"2018-07-30T10:34:49.000Z","updated":"2018-07-30T10:43:46.000Z","comments":true,"path":"2018/07/30/learning-long-term/","link":"","permalink":"https://crvdgc.github.io/2018/07/30/learning-long-term/","excerpt":"","text":"作者 Yoshua Bengio, Patrice Simard, and Paolo Frasconi 以下介绍中，块引用代表评论。 摘要 指出了 RNN 所面临的问题： temporal contingencies present in the input/output sequences span intervals ，也就是所谓的长依赖问题（long-term dependencies）。接下来指出问题的原因是基于梯度的训练方法。这种方法中存在 trade-off bbetween efficient learning by gradient descent and latching on information for long periods 。 基于此提出的解决方法是使用 alternatives to standard gradient descent ，也就是标准梯度下降外的替代品。 即使作为反向传播算法的提出者， Geoffrey Hinton 在 2017 年也对该算法提出了怀疑。不过近期又发了一篇 Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures ，在一些需要特殊网络结构的数据集上比较了生物学启发的多种训练算法，结果发现效果还是 BP 不好。这篇 1994 年的文章讲了 BP 如何不适合解决序列问题中的长依赖。值得一读。 引言 序列任务中需要系统能够存储、更新信息。从过去输入中计算得到的信息，对于输出是很有用的。 RNN 很适合这样的任务，因为其有内部状态（internal state）可以表示这样的上下文信息（context information）。这种特性来自于结构上的“循环”，静态的神经网络，即使引入延迟（Time Delay Neural Networks）也只能将信息储存特定时间长度，而不能储存不定时间长度。 RNN 的训练算法基于损失函数的梯度下降。比如 back-propagation through time (BPTT) 算法。 Forward propagation (FP) 算法计算代价更高，但可以在线学习。另一个训练受限 RNN 的算法中，动态神经元（dynamic neurons，只有向自己的一个反馈）和 FP 一样在时间上只需要本地信息，但权重更新所需计算只正比于权值数（和 BP 一样）。但其对于一般序列的存储能力有限，因此限制了其表示能力。 Long-term dependencies 的定义是在时间 t 的输出依赖于一个更早时间的 $\\tau \\ll t$ 。尽管 RNN 的表现超过很多统计网络，但更难训练到最优。局部最优的原因是次优解将短期依赖纳入了考虑，但没有考虑长期依赖。 Mozer [19] 发现反向传播很难发现长时间间隔的随机事件。本文从理论和实验上解释这个问题。 很惭愧只学过 BPTT ，对于另外两个都没有听说过。 一个含参数的动力系统（parametric dynamic system）的三个基本要求如下： 能够将信息储存任意时长 能够对抗噪声 参数可训练（训练时间合理） 定义信息锁存（information latching）为一个动力系统将特定比特的信息在状态变量中的长期存储。使用 hyperbolic attractor 的形式化定义将在 4.1 节给出。 hyperbolic attractor 本身的定义也将在第 4 节给出 文章共 5 节。第 2 节提出一个只有满足上述条件的系统才能解决的最小任务。接下来展示一个 RNN 解法，和一个负实验结果，表明梯度下降连这个简单任务都不适合。第 4 节的理论结果解释了一个系统要么稳定能够抵抗噪音，要么能使用梯度下降法高效训练，但不能同时达到。否则，在时间 t 的状态对于时间 0 的状态的导数将随 t 增大而指数减小。 因此，反向传播（以及一般的梯度下降算法）对于长序列效率低，因此不满足条件 3 。第 5 节提出了新的算法，并将其与反向传播的变体和模拟退火（simulated annealing）进行比较。使用的是输入输出依赖可以控制的简单任务。 第 2 节 说明问题的最小任务 该任务是一个序列二分类问题，最终的类别只取决于前 L 个输入值。 也即类别 $\\mathcal{C}(u_1,\\dots,u_T) = \\mathcal{C}(u_1,\\dots,u_L)$ 。而整个输入序列可以具有任意长度 $T \\gg L$ 。 该任务中，长度 L 之后的输入都是不相关的噪声。因此，模型需要有效地储存信息才能解决这个问题。本次实验中， L 之后的输入是均值为 0 的高斯噪声。 上面提到第 3 个标准是可学习性，这里有两个方面：第一，处理前 L 步的输入并正确分类。第二，将信息储存在状态变量中任意时长。在这个任务中，前面的分类和后面储存的时长无关，因此一个简单的解决方法是使用一个锁存子系统（latching subsystem），接收前面分类子系统的信息作为输入。 由于我们希望结果不与特定的分类任务相关（也就是与分类子系统相独立），因此我们只关注后面的锁存系统。一个锁存系统想要处理任意输入序列，就需要能将错误传播回前面的系统，并检测到引发锁存的事件。（propagate error information to a module that feeds the latching subsystem and detects the events leading to latching） 因此，我们将上面的任务修改如下：前 L 个输入可供输入算法调参（can be tuned by the learning algorithm）， L 步之后的输入是随机的高斯噪声。目标函数是二分类（期望输出值分别是 ±0.8）的平方误差和。 经过改造后， $h_t\\ (t=1,\\dots, L)$ 代表了类别信息的计算结果。直接学习 $h_t$ 比从输入中学习参数 $\\theta$ 容易。而且如果 $h_t$ 是对应时间步的输入 $u_t$ 的带参函数的话，也即 $h_t(u_t, \\theta)$ ，代价函数对于 $h_t$ 的导数是一样的（BPTT 下）。如果因为梯度消失导致 $h_t$ 不能被训练出来的话，那作为带参函数同样训练不出来。 研究锁存子系统的方法十分巧妙。锁存子系统想要达到预期的功能，至少要能将分类结果的正误信息传播回分类子系统。也即假设最后是 1 类，至少应该有方法将其通知到分类子系统。而我们观察其能否通知的方式，就是让其在前 L 步输入处还原最终的分类结果。对于原来的任务来说，我们通过训练参数让这个结果最终在 L 步处出现。而训练的依据就是在第 L 步我们接受到了锁存子系统传播回来的正确类别的信息。 举一个例子来说明。假设原来的任务 L = 3 ，接受到一个序列 101001… ，假设标签为第 1 类。分类系统应该在接受到 101 这三个时间步时就已经得出分类标签为 1 这个结论了。接下来，锁存系统将 1 这个结果储存起来，直至第 T 步输出。 然而我们只希望关注锁存子系统。在原任务中，如果最终标签是 1 ，锁存子系统应该能反向传播到第 3 个时间步，告诉分类子系统标签是 1 。因此，假如序列最终的标签是 1 ，我们希望无论前 3 个输入是什么，都能得到这一标签。所以锁存子系统只需要把真实标签传播给前 3 步的分类系统即可。假如没有分类系统，我们就让锁存子系统在前 3 个位置还原最终的标签。 也就是，假设序列的最终标签是 1 （对应目标值 0.8），锁存子系统应该在前 3 步输出 0.8, 0.8, 0.8 ，如果序列最终标签是 0 ，锁存子系统应该在前 3 步输出 -0.8, -0.8, -0.8 。假如我们有真的分类子系统，它应当能在这里拿到真实的标签，并据此将其与输入序列联系起来（通过调整带参函数的参数），比如 101 或者 010 等等。 个人感觉上面的切片测试方法不仅适合测试锁存，应该适合各种具有确定期望功能的子系统。举例来说，假设一个子系统需要进行 (+1) 这个运算，应当有能力对于输出 x 在输入处还原期望的输入 (x-1) ，这样才能通知前面的系统，我需要 (x-1) 。 当然这只是完成锁存或其他功能的必要条件。 关于导数相同一段，这是因为假设 h_t 仅取决于 u_t （而非其他时间步的输入），因此 h_t(u_t, \\theta) 是一个 context-free 的函数，根据求导的链式法则，对 h_t 的导数相同，而不论它是变量还是另一个变量的函数。 关于为何选取目标函数值为 0.8 ，因为 tanh 函数值域为 (-1, 1) ，而下一层的输入在 tanh(-1) = -0.76 到 tanh(1) = 0.76 之间。因此多个 tanh 单元叠加值域就在 (-0.8, 0.8) 之间。 第 3 节 一个简单的 RNN 解决方案 见图 Fig. 1a. 这是一个单一神经元的 RNN ，如果 $w \\gt 1$ ，有两个吸引子 $\\pm \\bar{x}$ ，值取决于 $w$ 。 假设初值 $x_0 = -\\bar{x}$ ，文献 [7, 8] 表明存在 $h^\\star \\gt 0$ 满足： $x_t$ maintains its sign if $|h_t| \\lt h^\\star$ ，也即小于阈值的输入不会改变状态的符号。 there exists a finite number of steps $L_1$ such that $x_{L_1} \\gt \\bar{x}$ if $h_t \\gt h^\\star \\ \\forall t \\le L_1$ 。也即假如超过正向阈值的输入持续了超过 $L_1$ 步后，会在 $L_1$ 步时将状态转到正向吸引子 $\\bar{x}$ 。 对于初值为正的情况有相应的对称结论。 当 $w$ 固定， $L_1$ 随着 $|h_t|$ 的增加而减小。 据此我们可以得到： 该系统可以储存一个 bit 的信息。通过最终输出的符号确定。 存储是通过将足够强（大于 $|h^\\star|$）的输入保持足够长的时间实现的。 小的（小于 $|h^\\star|$）噪声不会影响输出的符号。无论持续时间有多长。 参数 $w$ 也是可训练的，更大的 $w$ 对应于更大的阈值 $h^\\star$ ，对抗噪声的能力也就越强。 比如可以通过调整，使得 $h^1_t \\ge H$ 且 $h^0_t \\le H$ ，其中 $H \\gt h^\\star$ 来实现。 从上面的 Fig. 1b. 我们可以看到成功学习出来了加粗部分的前 3 个 $h_t$ 。 下面看各参数的影响。 首先 Fig. 2a 是噪声的标准差 $s$ 和初始的权值 $w_0$ 的影响。我们可以看到随着 $s$ 的增大和 $w_0$ 的减小，效果变差。 这很符合我们的直觉，噪声越强，对抗噪声的阈值越低，越容易丢失存储。 Fig. 2b 展示了随着 $T$ 的增加，收敛性变差。这表明，梯度下降即使对于长时间存储 1 位信息也很困难。 第 4 节 使用动力系统学习锁存 本节以基于动力系统的实时识别器为例子，说明 RNN 能够按双曲吸引子（hyperbolic attractors）的方式鲁棒性地储存信息的条件，会导致梯度消失的问题。 非自动（non-autonomous）的离散时间的系统，带有额外的输入： $$ a_t = M(a_{t-1}) + u_t $$ 和自动系统（autonomous system）： $$ a_t = M(a_{t-1})$$ 其中， $a_t$ 代表系统状态， $u_{t}$ 代表输入。两者是 $n$ 维向量， $M$ 代表非线性映射。 不带有额外输入的自动系统，可以通过引入额外状态变量和对应输入的方式，转变成非自动的系统。 比如 $a_t = N(a_{t-1}, u_{t-1})$ （ $a_t$ ， $u_t$ 分别为 $n$ 维和 $m$ 维向量）可以转化为 $a_t^\\prime = N\\prime(a_{t-1}\\prime) + u_t^\\prime$ ，其中 $a_{t}^\\prime = (a_t, y_t)$ 是一个 $n+m$ 维向量， $u_t^\\prime = (0, u_t)$ 即前 $n$ 个分量为 0 ，$N\\prime(a_t\\prime) = (N_t(a_{t-1}, y_{t-1}), 0)$ 即后 $m$ 个分量为 0 。 最终，$y_t = u_t$ 。 以上转换相当于将本来的内部状态变量当做系统的额外输入。使用映射计算出下面的 n 维状态后，就将剩下的 m 维分量丢弃。再从外界输入同样的 m 维分量，组合在一起恢复内部状态，作为下一次映射的输入。 注意到具有 $N^\\prime$ 形式的非自动系统也可以等价转换为自动系统。因此，不失一般性，我们只考虑非自动系统。 下面说明，当使用双曲吸引子进行锁存时，只有两种情况会发生：要么对噪声十分敏感，要么代价函数在 t 时刻对于 0 时刻的导数，将随 t 增加而指数下降。 4.1 分析 为了锁存一位信息，希望将系统的活动 $a_t$ 限制在定义域的一个子集 $S$ 上。这样能区分两个状态：在 $S$ 内，和不在 $S$ 内。为了使 $a_t$ 保持在其中，动力系统可以将其放在一个吸引子的吸引盆（basin of attraction）中。（吸引子也可以是子流形或子空间的吸引子）。想要“擦除”这一位信息，系统将 $a_t$ 从吸引盆中推出，可能放进另一个吸引盆中。本节说明，如果吸引盆是双曲的（hyperbolic），或者可以转化为双曲的（比如周期稳定吸引子 periodic stable attractor），那么对 $t_0$ 输入的导数会迅速消失。 定义 1 ： $E$ 是映射 $M$ 的不动点，如果 $E = M(E)$ 定义 2 ： 不动点点集 $X$ 是可微映射 $M$ 的双曲吸引子，如果 $\\forall a \\in X, \\ M^\\prime(a)$ 的特征值的绝对值小于 1 。 吸引子可能包含一个点（固定点吸引子， fixed point attractor），有限个点（周期性吸引子， periodic attractor）或者无限个点（混沌吸引子， chaotic attractor）。 一个稳定的固定点吸引子，对于映射 $M$ 是双曲的；一个稳定的周期性的吸引子，设其周期为 $l$ ，则对于映射 $M^l$ 是双曲的。 RNN 的吸引子的种类取决于权值矩阵。对于 $a_t = W\\ \\tanh(a_{t-1})+u_t$ ，如果 $W$ 是对称的，且其最小特征值大于 -1 的话，那么其所有吸引子都是固定点。如果行列式小于 1 或者系统是线性且稳定的，那么只有在原点处有一个固定点吸引子。 以上关于吸引子的知识全没有接触过。翻译了一下。仅从直观上进行理解。 定义 3 ： 一个吸引子 $X$ 的吸引盆 $\\beta(X)$ ，指映射 $M$ 下收敛于 $X$ 的点集。即 $\\beta(x) = { a\\ :\\ \\forall \\epsilon, \\exists l, \\exists x \\in X \\text{ s.t. } ||M^l(a)|| \\lt \\epsilon}$ 定义 4 ： $\\Gamma(X)$ 是双曲吸引子 $X$ 吸引盆中的 reduced attracting set，如果满足 $\\forall l \\ge 1$ ， $(Ml)\\prime(y)$ 的所有特征值小于 1。 根据定义有， $X \\subset \\Gamma(X) \\subset \\beta(X)$ 。 reduced 应该翻译成“剩余”还是“减小”？不太确定。这里只要求特征值小于 1 ，双曲吸引子要求特征绝对值小于 1 ，故双曲吸引子是 Gamma(X) 的子集。直觉上，特征值小于 1 可能对应上面“将其保持在吸引盆”中的要求。 定义 5 ： 一个系统可以鲁棒性地在 $t_0$ 锁存若干双曲吸引子中的一个吸引子 $X$ ，如果 $a_{t_0}$ 在 $X$ 对于定义自动系统的映射 $M$ 的 reduced attracting set 中。 对于非自动动力系统，只需 $a_t \\in \\Gamma(X) \\text{ for } t \\gt t_0$ 。接下来证明为什么使用 $\\Gamma(X)$ 来储存具有鲁棒性。 定理 1-3 及证明请查阅原文。最终证明了如果在 beta(X) 中，代表不确定性的球体会越来越大，因此输入的微小扰动可能将轨迹引导进入一个错误的吸引盆，即系统无法对抗噪声。相反，如果在 Gamma(X) 中，则能在输入中找到一个界，保证 a_t 一直在 X 中的点的特定距离内。因此是鲁棒的。见 Fig. 3 。 定理 4 ： 当输入 $u_t$ 使得系统在时间 0 后保持在 $X$ 上鲁棒时，随着 $t$ 趋近于无穷，$a_t$ 对 $a_0$ 的偏导趋近于 0 。 也就是说对抗噪声的代价是对过去事件的导数与近期事件相比会小很多。 4.2 对权重梯度的影响 $$\\frac{\\partial C_t}{\\partial W} = \\sum_{\\tau \\le t} \\frac{\\partial C_t}{\\partial a_\\tau}\\frac{\\partial a_\\tau}{\\partial W} = \\sum_{\\tau \\le t} \\frac{\\partial C_t}{\\partial a_t}\\frac{\\partial a_t}{\\partial a_\\tau} \\frac{\\partial a_\\tau}{\\partial W}$$ 因此，相对于较近的事件，$\\tau \\ll t$ 的前两项的乘积较小，因此对最终的结果影响较小。也就是，即使可能存在一个 $W$ 使得 $a_\\tau$ 进入一个更好的吸引盆，但对 $W$ 的梯度不会反映这种可能性。 举例来说，假设 A ， B 两个系统顺次相接完成一项任务。且要求 B 使用 A 在 0 时刻检测到事件的信息，在遥远的 T 时刻使用该信息计算错误。（第 2 节定义的任务符合这个特征）。如果 B 训练不足，不能将 A 的结果锁存，那么 T 时刻的错误对 A 在 0 时刻产生的结果影响非常小。相反，如果 B 能够将信息储存很长时间，正确的梯度会被传播回去，但却迅速消失成为小值。因此， A 很难训练。 第 5 节 替代的方法 本节中给出了模拟退火等算法作为梯度下降的替代算法并在多个任务上测试了结果。每个任务上都有算法比反向传播更佳。 第 6 节 结论 一个未进行讨论的点是类似的问题是否会在混沌吸引子中出现。 这个问题可能也会在深度前馈神经网络（feedforward network）中出现，因为 RNN 按时间展开就是一个共享权值的前馈神经网络。 本文研究并不意味着不能为特定任务训练神经网络，相反，如果有先验知识可以设置神经网络的权值共享和初值，利用起来会提升效果。比如在 latch problem 和 parity problem 中，先使用短序列进行训练可以让系统迅速进入正确的区域。","categories":[{"name":"计算机","slug":"计算机","permalink":"https://crvdgc.github.io/categories/计算机/"},{"name":"人工智能","slug":"计算机/人工智能","permalink":"https://crvdgc.github.io/categories/计算机/人工智能/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"https://crvdgc.github.io/tags/RNN/"},{"name":"长依赖","slug":"长依赖","permalink":"https://crvdgc.github.io/tags/长依赖/"}]},{"title":"编程解决《去月球》 To the Moon 记忆碎片小游戏","slug":"memento-solver","date":"2018-07-28T17:24:21.000Z","updated":"2018-07-30T07:20:16.000Z","comments":true,"path":"2018/07/29/memento-solver/","link":"","permalink":"https://crvdgc.github.io/2018/07/29/memento-solver/","excerpt":"","text":"引言 记忆碎片（Memento）是游戏《去月球》（To the Moon）中的一个解谜小游戏。基本规则已在游戏中介绍。即通过点击按钮翻转某一行、列或者对角线，直至所有碎片都转成同一面。 这个解谜与成就/剧情分支无关，而且谜题设置比较简单，比较适合自己玩。谜题也非随机生成，答案都可以在网上找到。 兴趣使然地写了一个编程通用解法。在此介绍一下。 分析 注意到同一位置进行任意偶数次变换都等价于不变，而奇数次变换等价于进行 1 次。为使总变换次数最少，只需决定每个位置是否进行变换即可。 其次，多个不同变换之间满足交换律和结合律，即任意改变变换之间的顺序不会影响最终的结果。 因此，每个解决方案，对应到将变换集合映射到 {进行，不进行} 集合的一个函数映射。若有 N 种可能的变换，则共有 2^N 种可能的解决方案。 可以使用二进制编码，宽度为 行数 + 列数 + 1 。每一位都对应着翻转行、列或对角线。该位为 1 代表进行变换，该位为 0 代表不进行变换。 接下来我们只需要枚举所有可能的解决方案，并将其中正确的挑选出来即可。 游戏中给出了最佳步数，因此可以作为加速搜索的方式，一旦解决方案中，变换的次数超过最佳步数，则跳过。这样，整个搜索空间大小从 2^N 减小到了 C(N, B) ，其中 B 是最佳变换次数， C 是组合数。 实现 首先处理输入，我们将输入的图形转换为一个布尔矩阵。 12345rowNum = int(input('Row number: '))colNum = int(input('Col number: '))best = int(input('Best move: '))print(\"Input table, 1 for solved, 0 for unsolved\")table = [list(map(lambda x: True if int(x) == 1 else False, input().split())) for i in range(rowNum)] 接下来定义一些工具函数： 12345def check(): return all([all(row) for row in table])def flip(i, j): table[i][j] = False if table[i][j] else True 我们需要从解决方案中提取对应的行、列、对角线进行翻转： 12345678910111213def apply_solution(solution): for r in range(rowNum): if solution[r]: for c in range(colNum): flip(r, c) for c in range(colNum): if solution[rowNum+c]: for r in range(rowNum): flip(r, c) if solution[-1]: # diagnol, from left-bottom for i in range(min(rowNum, colNum)): flip(rowNum-1-i, i) 注意，对角线是从左下角开始的。 类似的，我们定义打印解决方案的函数： 123456789def print_solution(solution): for r in range(rowNum): if solution[r]: print('r%s' % r) for c in range(colNum): if solution[rowNum+c]: print('c%s' % c) if solution[-1]: print('d') 注意，编号从 0 开始，方向是从上到下、从左到右。 d 代表对角线。 主要过程是枚举整个解决方案空间： 1234567891011for solution in itertools.product([False, True], repeat=rowNum+colNum+1): if solution.count(True) &gt; best: continue apply_solution(solution) if check(): print_solution(solution) break else: apply_solution(solution)else: print('No answer for best move %s' % best) 首先，使用 itertools.product 产生所有的编码。 对于每一个编码，如果变换数大于最佳，则跳过当前。 如果找到了一个解，则打印并跳出，否则重新调用 apply_solution 函数将表格恢复原状态。 测试 123456789101112131415161718192021Row number: 3Col number: 3Best move: 2Input table, 1 for solved, 0 for unsolved1 0 01 0 01 0 0c1c2Row number: 3Col number: 3Best move: 3Input table, 1 for solved, 0 for unsolved1 0 00 0 00 0 1r1c1d 附注 itertools.product 该函数产生一系列 iterable 的笛卡尔积（Cartesian product），如果指明了 repeat 关键字参数，则会将前面所有的 iterable 再和自己进行笛卡尔积。 全局依赖 由于对角线的存在，每一次枚举行/列无法完全确定其他某个变换是否进行，因此没有进一步减小空间的机会。 但事实上，除了第一个谜题，游戏中每个谜题都包含对角线。因此，可以默认只在奇数编码中搜索。将对角线放在最后一位也有利于快速找到解决方案。 完整代码 gist 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import itertoolsdef solve_memento(): rowNum = int(input('Row number: ')) colNum = int(input('Col number: ')) best = int(input('Best move: ')) print(\"Input table, 1 for solved, 0 for unsolved\") table = [list(map(lambda x: True if int(x) == 1 else False, input().split())) for i in range(rowNum)] def check(): return all([all(row) for row in table]) def flip(i, j): table[i][j] = False if table[i][j] else True def apply_solution(solution): for r in range(rowNum): if solution[r]: for c in range(colNum): flip(r, c) for c in range(colNum): if solution[rowNum+c]: for r in range(rowNum): flip(r, c) if solution[-1]: # diagnol, from left-bottom for i in range(min(rowNum, colNum)): flip(rowNum-1-i, i) def print_solution(solution): for r in range(rowNum): if solution[r]: print('r%s' % r) for c in range(colNum): if solution[rowNum+c]: print('c%s' % c) if solution[-1]: print('d') for solution in itertools.product([False, True], repeat=rowNum+colNum+1): if solution.count(True) &gt; best: continue apply_solution(solution) if check(): print_solution(solution) break else: apply_solution(solution) else: print('No answer for best move %s' % best)solve_memento()","categories":[{"name":"计算机","slug":"计算机","permalink":"https://crvdgc.github.io/categories/计算机/"},{"name":"杂谈","slug":"计算机/杂谈","permalink":"https://crvdgc.github.io/categories/计算机/杂谈/"}],"tags":[{"name":"游戏","slug":"游戏","permalink":"https://crvdgc.github.io/tags/游戏/"},{"name":"python","slug":"python","permalink":"https://crvdgc.github.io/tags/python/"},{"name":"兴趣","slug":"兴趣","permalink":"https://crvdgc.github.io/tags/兴趣/"}]},{"title":"PKD 电子梦评论 - Real Life / Exhibit Piece","slug":"electricDreams-01","date":"2018-07-25T07:22:46.000Z","updated":"2018-07-30T07:06:43.000Z","comments":true,"path":"2018/07/25/electricDreams-01/","link":"","permalink":"https://crvdgc.github.io/2018/07/25/electricDreams-01/","excerpt":"","text":"电子梦系列 PKD 的作品，改编成为电视剧、电影的不在少数。但我还是对《电子梦》抱有独特期待的。不同于以往短篇改电影（《少数派报告》），长篇改电影（《银翼杀手》），长篇改电视剧（《高堡奇人》），《电子梦》将 PKD 的短篇改编成单元剧。每一篇小说独立，每一集剧也独立。短篇小说是最能体现科幻“点子小说”特点的载体。没有了人物和情节的负担，小说可以尽情发挥某个点子设定。身为 PKD 迷，怎么能错过这部剧呢？ 不过看了剧之后，感觉有几集水平真心一般。故从攒着买全集的钱里拨款，买了一本对应小说集，看看原作是否如此。本系列博客对应于每一集每一篇的评论。 &lt;&lt;&lt;&lt; 主要剧透预警 &gt;&gt;&gt;&gt; Quick Recap Real Life 剧集的主角有两位，一位是 Sarah ，未来世界的女同警察，一次针对警察的屠杀的幸存者，另一位是 George ，生活在现代的布鲁斯韦恩——拥有巨额财产却热衷于亲自行侠仗义打击犯罪。准确来讲，两个人共享思想和记忆，因此只是一个人的两个身份——问题在于，哪个身份是真的？ 两人分别在自己的世界中，通过 VR 设备进入到另一个世界和身份中。他们的爱人 Katie 名字相同，长相也相同，他们都在追查同一个姓名的罪犯。甚至来到同一家餐厅吃着同样的汉堡。区别在于，未来世界的生活“too good to be true”。在现代世界中， George 的妻子被罪犯报复残忍杀害，追捕罪犯也以失败告终。未来世界中， Sarah 不仅家庭美满，也成功抓到了罪犯。 两个 VR 项目都是捕捉潜意识中的愿望，将其转化为体验的。一个世界是以另一个世界为基础想象出来的。 如果 Sarah 的身份是虚假的，那么这个未来世界仅仅是一个“科幻世界”，里面有飞车、女同、幸福的家庭、成功抓捕到了罪犯，一切都是为了缓解丧妻之痛的幻想。 如果 George 的生活是虚假的，现实世界是 Sarah 的赎罪场，用自己的幻想折磨自己，以此发泄自己不配有这样幸福生活的自卑和愧疚——幸存者常有的心理障碍 survivor’s guilt，凭什么我能活下来。 对于 Sarah 的过去有清晰的记忆，而 George 的过去则需经人提醒才能慢慢想起。最终，他/她选择了 George 的生活，打碎了 VR 设备。未来世界的妻子看着自己的爱人迷失在 VR 世界中——原来这个世界才是真实的。 “为什么她选择了那样的人生？” “因为她想被自己的罪过惩罚，无论是真实的，还是虚构的。” Exhibit Piece 根据 wiki 中的链接，本篇小说已经进入公共领域了。这里可以找到电子版和PDF版。 小说的主角 George Miller 是未来高压社会下的一名历史研究员，专门研究 20 世纪 50 年代的历史（本篇小说最初发表时间 1954 年）。为了研究的氛围，他平时身着那个年代的服装（在未来则属于奇装异服）。在一次和上司的冲突后， 他被展品——一个 50 年代生活的复制品——中的声音所吸引而进入了展品中。在那里，他回忆起了自己在其中的生活，一个妻子，两个孩子，有些令人担心的上司。 他感到困惑，因此找到了过去世界中的心理医生 Grunberg ，对方认为两套理论同样有说服力，但 George 却提出了第三个理论——两个世界都是真实的。医生建议他回到自己在世界中移动的点上，这样就能确定哪个世界是真实的。 他在那里见到了自己未来世界的同事和上司，他们认定 George 患了狂想症，威胁他要将他安乐死，并将展品拆除。 George 认为未来的高压社会没有什么值得生活的。与之相比，50 年代竟然成为了自由的黄金年代。 George 表示，展品实际上是一个时空门，他通过它回到了过去，而其他人无法回去。拆除展品只是毁坏了时空门，他将永远留在过去——正是他所希望的。 当 George 回到家中拿起报纸时，却发现上面写着，俄罗斯展示钴炸弹，全世界将面临毁灭。 究竟未来世界是他的幻想，还是他真的相信自己的展品是一个真实的世界，因为展品的拆除才出现了炸弹的新闻。抑或他真的回到了过去，炸弹的出现才是导致未来社会高压的原因。 评论 改编剧中几乎没有留下什么，但两篇的主题都是“变换的现实”——一个 PKD 作品中经常出现的主题。如果我们对于现实的认知不能超过感官所及，那么又怎能区分“真实的现实”和“虚拟的现实”。《黑客帝国》中探讨过相同的主题，然而这一篇与《攻壳机动队》中更进一步，如果连思想和记忆都可以进行虚拟，不仅世界的真实性变得不可靠，连自我的真实性都无法决定了。 看过《银翼杀手》的同学一定会对此感到熟悉，仿生人通过虚假记忆不知道自己是仿生人，这是一个核心设定，《银翼杀手2049》中，仿生人知道自己是仿生人，且知道自己有虚假的记忆，一下将揭示虚假记忆所展现的力量削弱到几乎没有了。 PKD 自己也经常怀疑所处的世界是否为真实，一如他笔下的人物，他又无法最终抉择究竟哪种世界为真，因此作品的结局常常是含糊的。改编中给出了明确的答案，因此转变成了对虚拟现实这一万能许愿机的警告。 此外，在面对多个可能的真实时，人总是需要选择一种“自己的真实”，也即相信哪一个真实才是“真实的真实”。在选择的过程中，一种标准是，一定要活在真实中，也就是选择最像真实的那个真实。另一种标准是，只要活在自己最喜欢的真实即可。除了选中的真实以外，其他世界中的人都会觉得你在有意无意地“欺骗自己”。 虽然两个故事都没有展示，不过发现自己的选择错误也是一个有趣的点。比如剧中，与虚拟人物热情相拥，但却看到整个世界渐渐陷入黑暗，会怎么想呢？后悔自己的决定？还是坚持自己的选择？ PKD 提出了问题，意识到自己没有能力去回答，因此他提出了更多的问题。 《电子梦》的执行出品人 Ronald D. Moore （《太空堡垒卡拉狄加》、《星际迷航7》、《星际迷航8》、《星际迷航：下一代》）为本篇写了导读。他在其中表达了对今年来虚拟现实技术的关注，因此将 VR 项目引入到了故事中。实际上，他自己作为编剧写过一部探讨虚拟现实的电影 Virtuality 。 PKD 的时代已经出现了 VR 技术的先声，尽管 PKD 并没有明确的展示使用何种设备进入虚拟的现实。但他的作品无疑谈及了更广泛意义上的模拟现实（Simulated Reality），甚至不是模拟，而是另一种完全不同的现实。","categories":[{"name":"科幻","slug":"科幻","permalink":"https://crvdgc.github.io/categories/科幻/"},{"name":"评论","slug":"科幻/评论","permalink":"https://crvdgc.github.io/categories/科幻/评论/"}],"tags":[{"name":"pkd","slug":"pkd","permalink":"https://crvdgc.github.io/tags/pkd/"},{"name":"剧集","slug":"剧集","permalink":"https://crvdgc.github.io/tags/剧集/"},{"name":"cyberpunk","slug":"cyberpunk","permalink":"https://crvdgc.github.io/tags/cyberpunk/"},{"name":"虚拟现实","slug":"虚拟现实","permalink":"https://crvdgc.github.io/tags/虚拟现实/"}]},{"title":"为字体添加powerline支持","slug":"为字体添加powerline支持","date":"2018-05-04T01:10:52.000Z","updated":"2019-04-25T17:41:42.757Z","comments":true,"path":"2018/05/04/为字体添加powerline支持/","link":"","permalink":"https://crvdgc.github.io/2018/05/04/为字体添加powerline支持/","excerpt":"","text":"vim-airline是一款不错的vim实用工具。还可以通过定制主题进行美化。 看到一款不错的主题安装后，却出现了奇怪的情况。 这效果完全不一样好吗！ 查阅资料得知，这是字体的问题。 众所周知， Vim 是一个以命令行为主的编辑器，我用的 neovim 更为如此。那么如何在命令行中画出 UI 需要的图形呢？这里采取的方式是，使用绘图字符（ glyph ，又称字形）。比如「■」的字符。如果这个字符在当前环境下未定义，或者定义成别的了，就会出现上面的错误。 Vim 原来有一个基于 Python 的项目叫 powerline （现已停止开发），可算作 vim-airline 的精神前身吧。为了画出美观的 UI ，自己定义了一部分绘图字符，把它们放在了没有使用的 Unicode 编码中。 后来大家发现了绘图字符在命令行下绘制 UI 的简洁程度（最基本的只有 7 个），于是很多命令行应用也使用这些字符来绘制自己的 UI 了。而这些字符就被统称为 powerline glyphs 。 后来有人仿照这个，继续扩展命令行字符集，你可以在 nerdfonts 中找到 40 多种， 3000 多个字符。正如名字暗示的那样，里面包含了各种各样的 nerd stuff ，比如各种技术标志，甚至还有 Spock 手。如果 emoji 由 nerds 发明的话，大概就是这个效果吧。 由于这些字符不是标准编码，想要使用这些字符，需要字体在这些编码上正确定义。比如 powerline/fonts 给出了很多打好补丁的字体。 如果纯英文环境的话，常见的字体都包括了。但代码有中英混排需求，而中文字体又少有打好补丁的。这时可以选择上面 nerdfonts 提供的打补丁程序，给自己的字体打上补丁。 注意运行完成后得到的字体会被更名，以和原字体区分开。一般会以 Nerd Font 结尾。 如果不想更改各种软件的设置的话，可以通过编辑字体元信息使得名称和原字体一样。并用新字体将原字体覆盖。 最终得到的效果如下： 我平常使用 Consolas-with-Yahei 字体，打好补丁的版本存放在 crvdgc/Consolas-with-Yahei ，欢迎使用。","categories":[{"name":"计算机","slug":"计算机","permalink":"https://crvdgc.github.io/categories/计算机/"},{"name":"工具","slug":"计算机/工具","permalink":"https://crvdgc.github.io/categories/计算机/工具/"}],"tags":[{"name":"字体","slug":"字体","permalink":"https://crvdgc.github.io/tags/字体/"}]}]}